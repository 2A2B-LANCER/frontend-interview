## HTTP

------

### HTTP常见面试题

------

#### OSI 七层模型

- 应用层：为应用程序提供交互服务，DNS/HTTP/SMTP/SNTP
- 表示层：负责数据格式的转换
- 会话层：负责在网络中两个节点之间建立、维持、终止通信
- 传输层：负责网络中的数据传输服务，TCP/UDP
- 网络层：选择合适的路由和节点，IP
- 数据链路层：把网络层的数据包组装成帧，并在相邻节点的链路上传输帧
- 物理层：实现相邻节点比特流的透明传输



#### HTTP基本概念

------

超文本传输协议

> HTTP 是一个在计算机中专门在 **两端** 之间传输 文本、图片、音频、视频等 **超文本** 数据的 **约定和规范**

主要从三方面来理解

1. 超文本：字面意思，就是能够传输的数据不只有文本，现在文本的含义已经可以扩展为图片、视频、压缩包等了，**超的关键是超链接**，用于从一个超文本跳转到另一个超文本
2. 传输：端对端进行数据搬运，两个特点：双向，可中转
3. 协议：某个行为的两个以上的参与者用来制约大家行为的约定和规范



#### HTTP常见状态码

------

|      | 具体含义                                           | 常见状态码      |
| ---- | -------------------------------------------------- | --------------- |
| 1xx  | 提示信息，表示目前处于协议处理的中间状态           |                 |
| 2xx  | 成功，已经收到报文并被正确处理                     | 200/204/206     |
| 3xx  | 重定向，资源位置发生变动，需要重新向服务器发起请求 | 301/302/304     |
| 4xx  | 客户端错误，请求报文出错                           | 400/403/404     |
| 5xx  | 服务端错误，服务器在处理请求时内部出错             | 500/501/502/503 |

- `2xx` 成功
- `200 OK`：一切正常，除了 `HEAD` 请求，都会有 `body` 数据
- `204 No Content`：与 200 基本相同，没有 `body` 数据
- `206 Partial Content`：HTTP分块下载或断点续传，表明当前返回的 `body` 数据不是资源的全部
- `3xx` 重定向
- `301 Moved Permanently`：永久重定向，
- `302 Found`：临时重定向，301和302都会在响应头里使用字段 `Location`，指明后续要跳转的 URL
- `303 See Other`：重定向到其他资源，重定向后必须使用 `GET` 方法
- `304 Not Modified`：协商缓存命中，这代表服务器端数据没有更新，浏览器直接使用缓存中的数据
- `307 Temporary Redirect`：类似 `302` 的临时重定向，请求方法不得更改
- `308 Permanent Redirect`：类似 `301` 的永久重定向，请求方法不得更改
- `4xx` 客户端发送的报文有误
- `400 Bad Request`：客户端请求的报文有错误
- `401`：未授权，需要登录
- `403 Forbidden`：服务器禁止访问资源，请求没错
- `404 Not Found`：请求的资源在服务器未找到
- `405`：禁用请求中指定的方法
- `5xx` 服务器处理请求时内部出错
- `500 Internal Server Error`：服务器出错
- `501 Not Implemented`：客户端请求的功能还未实现
- `502 Bad Gateway`：通常是服务器作为⽹关或代理时返回的错误码，表示服务器⾃身⼯作正常，访问后端服务器 发⽣了错误
- `503 Service Unavailable`：服务器繁忙



#### HTTP 请求方法

| 请求方法 | 描述                                                   |
| -------- | ------------------------------------------------------ |
| GET      | 获取服务端资源                                         |
| HEAD     | 和 GET 一样，但是值传递头部信息                        |
| POST     | 更新、创建资源；非幂等，一般指向资源集合               |
| PUT      | 更新、创建资源；幂等，一般指向具体单一资源             |
| DELETE   | 删除资源                                               |
| PATCH    | 修改部分资源                                           |
| CONNECT  | HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 |
| TRACE    | 回显服务器收到的请求                                   |



#### HTTP 常见字段

| 字段名           | 作用                                                         |
| ---------------- | ------------------------------------------------------------ |
| Host             | 指定服务器的域名                                             |
| Content-length   | 本次相应的数据长度                                           |
| Connection       | 常⽤于客户端要求服务器使⽤ TCP 持久连接(keep-alive)，直到客户端或服务器主动关闭连接 |
| Content-Type     | ⽤于服务器回应时，告诉客户端，本次数据是什么格式             |
| Accept           | 客户端请求的时候，⽤以声明⾃⼰可以接受哪些数据格式           |
| Content-Encoding | 表示服务器返回的数据使⽤了什么压缩格式                       |
| Accept-Encoding  | 客户端在请求时，⽤以说明⾃⼰可以接受哪些压缩⽅法             |



#### GET 与 POST 的区别？

- GET 方法用于从服务器获取资源
  - 参数写在 URL 中
  - 参数长度无限制，但是浏览器会限制 URL 长度
  - 默认缓存
  - 只能进行 URL 编码
  - 发送一个数据包，header 和 body 一起发送

- POST 方法用于对服务器的资源进行修改
  - 参数写在 请求体中
  - 参数长度无限制
  - 默认不缓存
  - 任意编码都可以
  - 发送两个数据包，header 先发送，服务端返回 100 continue 后，浏览器再发送 body 数据



#### GET 和 POST 方法 是 安全和幂等的吗？

> 安全：在 HTTP 协议中，是指请求方法不会 **破坏** 服务器上的资源

> 幂等：多次 **执行相同的操作**，**结果都是相同的**

- GET
  - 因为 GET 方法是 **只读** 操作，所以肯定 **是安全的**
  - 一般意义上来讲，GET 方法 **是幂等的**，因为 GET方法是用来从服务器获取资源的只读操作，它不应该改变服务器上的资源
- POST
  - POST 方法是 **新增或提交数据的** 操作，会修改服务器上的资源，所以 **不是安全的**
  - 每一次提交都会修改服务器上的资源，所以执行相同的操作，结果也不同，**不是幂等的**



#### HTTP/1.1 的优点有哪些，怎么体现的？

1. 简单：HTTP 基本的报文格式是 `header + body`，头部信息也是 `key-value`形式的简单文本，易于学习
2. 灵活和易于扩展：请求方法多种多样；URL/URI 除了少数几个参数是固定的，大部分是自定义的；状态码除了常用的还可以自己定义；头部信息也是可以自定义；等等，HTTP协议中很多都允许开发人员 **自定义和扩充**；HTTP 位于 应用层，下层可以随意变化；比如 HTTPS 就是在 HTTP 和 TCP 之间建立了 SSL/TLS 安全传输层；HTTP3 甚至把 TCP 换成了基于 UDP 的 QUIC
3. 应用广泛和跨平台：各种用户端都在使用，天然跨平台

#### HTTP/1.1  的缺点有哪些？

- 无状态
  - 好处：不需要额外的资源记录状态信息，减轻服务器压力
  - 坏处：服务器没有记忆，在完成有关联性的操作的时候会非常麻烦；
  - 坏处的解决方案之一：Cookie，通过在请求和响应报文中写入 Cookie 来控制客户端的状态
- 明文传输
  - 好处：方便阅读，浏览器的 F12 控制台或 Wireshark 抓包都可以直接查看
  - 坏处：HTTP 的所有信息都是暴露下的，多阶段的传输可能会导致信息泄露
- 不安全（**缺点**）
  - 明文传输，信息可能会泄露
  - 不验证通信方的身份，可能遭遇伪装
  - 无法验证报文的完整性，有可能已经被篡改了

**安全问题如何解决？**

HTTPS，通过引入 SSL/TLS 安全传输层来解决

#### HTTP/1.1 的性能如何？

HTTP 协议基于 **TCP/IP**，使用 **请求-应答** 的通信模式

- 长连接（TCP 持久连接）

一般的 HTTP 请求（短连接），是**每发起一次请求都要进行一次 TCP 连接（三次握手），而且是串行请求**，这无疑会增加通信开销

长连接就是解决这个问题的，它有效的**减少 TCP 连接的重复建立和断开造成的额外开销**，减轻了服务器端的压力

特点是：**只要任意一端没有明确断开连接，就保持 TCP 连接的状态**

- 管道网络传输

长连接使得管道网络传输成为了可能

短连接是这样的：

1. TCP 建立连接
2. 请求A发送
3. 请求A响应
4. TCP 断开连接
5. TCP 建立连接
6. 请求B发送
7. 请求B响应
8. TCP 断开连接

B请求发送之前必须等 A 请求完成响应，并且断开TCP连接，然后重连

长连接的普通形式：

1. TCP 建立连接
2. 请求A发送
3. 请求A响应
4. 请求B发送
5. 请求B响应
6. TCP 断开连接

这样的 长连接 解决的问题是 TCP 断开重连的性能额外开销，但是，**请求B的发送仍旧需要等待 请求A 的响应完成**

**管道机制**

1. TCP 建立连接
2. 请求A发送
3. 请求B发送
4. 请求A响应
5. 请求B响应
6. TCP 断开连接

请求 A 和 请求 B 可以同时发送，**但是响应依旧是按照请求发送顺序的**

要是前⾯的回应特别慢，后⾯就会有许多请求排队等着。这称为 **队头堵塞**。

- 队头阻塞

**请求 - 应答** 的模式加剧了 HTTP 的性能问题。 因为当顺序发送的请求序列中的⼀个请求因为某种原因被阻塞时，在后⾯排队的所有请求也⼀同被阻塞了，会招致 客户端⼀直请求不到数据，这也就是 **队头阻塞**。好⽐上班的路上塞⻋



#### HTTP 与 HTTPS 的区别？

1. HTTP 是明文传输，有安全风险，HTTPS 在 HTTP 和 TCP 之间加入了 SSL/TLS 安全协议，使得报文能够加密传输
2. HTTP 连接建立相对简单，TCP 三次握手之后就可以进行 HTTP 的报文传输；HTTPS 在 TCP 三次握手之后，还需要进行 SSL/TLS 的握手过程，才能进行加密报文的传输
3. HTTP 的端口号是 80；HTTPS 的端口号是 443
4. HTTPS 协议需要向 证书权威机构（CA）申请数字证书，来保证服务器的身份是可信的



#### HTTPS 解决了 HTTP 的哪些问题？

1. 窃听风险：因为是明文传输而导致的
2. 篡改风险：因为无法验证报文完整性（也是因为明文传输）
3. 冒充风险：无法验证通信方身份

加入 SSL/TLS 协议之后，解决了以上问题：

1. 信息加密：窃听到信息之后，也没办法解密
2. 校验机制：通信内容篡改之后就无法正常显示
3. 身份验证：解决冒充问题



#### HTTPS 是如何解决 HTTP 的问题的？

1. **混合加密** 实现信息的 **机密性**，解决窃听问题

> 混合加密就是结合使用 对称加密 和 非对称加密

- 在通信建立前采用 **非对称加密** 的方式交换 **会话秘钥**
- 通信过程中使用 **对称加密** 的方式进行加/解密明文数据

为什么采用混合加密？

- 对称加密使用一个秘钥，**运算速度快**，但是 **无法做到安全的密钥交换**
- 非对称加密使用 公钥和私钥，**解决了密钥交换问题但是速度慢**

2. **摘要算法** 实现数据的 **完整性**，防止数据被篡改

客户端发送的加密数据包含两部分：明文 和 摘要

服务端解密后，用相同的摘要算法计算获得的明文，再把结果和接收的摘要进行比较，如果相同则说明数据完整

3. 把服务器公钥放入到 **数字证书**，解决冒充风险

数字证书包含 **服务器公钥 和 CA 数字签名**

客户端通过 CA的公钥检验数字证书的真实性，如果是真实的（说明通信方的身份是有效的），就使用 数字证书中的服务器公钥加密报文，这样就解决了冒充的风险



#### HTTPS 是如何建立连接的？SSL/TLS1.2 四次握手阶段交互了什么？

SSL/TLS 协议基本流程：

- 客户端向服务器索要并验证服务器的公钥
- 双方协商生成 **会话秘钥**
- 双方使用会话秘钥进行加密通信

前两步就是 SSL/TLS 的建立过程（握手阶段）

SSL/TLS 的握手阶段涉及四次通信

1. Client Hello

   由客户端向服务器发起加密通信请求，也就是 `Client Hello` 请求

   主要包含以下信息：
   1. 客户端支持的 SSL/TLS 协议版本
   2. 客户端生成的 第一个**随机数（Client Random）**，用于生成 **会话秘钥**
   3. 客户端支持的加密方法列表

2. Server Hello

   服务端收到 `Client Hello` 请求后，向客户端发出响应 ` Server Hello`

   主要内容如下：

   1. 确认的 SSL/TLS 协议版本，如果浏览器不支持，则关闭加密通信
   2. 服务端生成的 第二个 **随机数（Server Random）**，用于生成 **会话秘钥**
   3. 确认的加密方法列表
   4. 服务器的数字证书

3. 客户端回应

   客户端接收到 `Server Hello` 之后，首先通过 CA 公钥，确认数字证书的真实性；如果数字证书确认无疑，客户端会从数字证书中获得服务器公钥，用于加密以下报文

   1. 第三个 **随机数（pre-master key）**，依旧用于生成 **会话秘钥**
   2. **加密通信算法改变通知**，这次之后客户端发起的通信都是使用 **会话秘钥** 对称加密的
   3. **客户端握手结束通知**，以及之前所有内容的发生的数据做个摘要，供服务端校验

   **三个随机数**，使用双方商定的**加密算法**，**各自生成** 之后通信用的 **会话秘钥**

4. 服务端最后的回应

   服务器端收到客户端回应的密文，使用 **私钥** 解密，得知第三个随机数，使用加密算法，算出 **会话秘钥**，然后向客户端发送最后信息：

   1. **加密通信算法改变通知**，表示之后的响应信息都是使用 **会话秘钥** 对称加密的
   2. **服务器握手结束通知**，以及之前所有内容的发生的数据做个摘要，供客户端校验

至此，SSL/TLS 握手阶段结束。接下来的通信都是使用 **会话秘钥** 进行加密的



#### HTTP/1.1 相比 HTTP/1.0 性能上的改进

- TCP 长连接 改善了 短连接的时候，TCP连接频繁断开重连造成的性能开销
- TCP 长连接使得管道网络传输得以实现，第一个请求发出，不必等待响应就可以发出第二个请求，一定程度上减少了整体的响应时间



#### HTTP/1.1 的性能瓶颈

- 请求/响应头部未经压缩就发送，头部信息越多，延迟越大，只能压缩 body 部分
- 每次互相发送相同的头部信息会造成额外的性能浪费
- 队头阻塞，因为服务端是按照请求顺序进行相应的，如果某个响应很慢，后面的响应都会阻塞
- 没有请求优先级控制
- 请求只能从客户端开始，服务端只能被动响应



#### HTTP/2 对 HTTP/1.1 的性能瓶颈做出了什么优化？

HTTP/2 是基于 HTTPS 的，所以 HTTP/2 的安全性是有保障的，而且HTTP/1.1之后，所有通信默认使用长连接

改进：

- 头部压缩

  如果同时发出多个请求，头部一样或者相似，HTTP/2 **会消除重复的部分**

  也就是 `HPACK` 算法：客户端和服务端同时维护一张头信息表，所有字段存入这张表中，生成一个索引号，同样字段不再发送，只发送索引号，这样速度就提高了

- 二进制格式

  HTTP/1.1 的报文是文本形式的，HTTP/2 使用 **二进制**，头信息和数据体都是二进制，统称为帧（frame）：头信息帧和数据帧。这样省略了服务端文本转二进制的过程，**增加了数据传输效率**

- 数据流

  HTTP/2 的数据包也不是按照顺序发送的了，**同一个连接 中连续的数据包，可能属于不同的请求（响应）**，因此需要对数据包做标记，指出它属于谁

  **一个请求或响应的所有数据包，统称为一个数据流（Stream）**，每个数据流都有一个唯一编号；客户端发出的数据流编号为奇数，服务端发出的为偶数

  客户端还可以 **指定数据流的优先级，高优先级先响应**

- 多路复用

  HTTP/2 可以 **在一个连接中并发多个请求或相应，不用按照顺序一一对应**

  移除了 HTTP/1.1 的串行请求，就不会有 **队头阻塞** 的问题了，降级了延迟，大幅提高了连接利用率

  举例来说，在⼀个 TCP 连接⾥，服务器收到了客户端 A 和 B 的两个请求，如果发现 A 处理过程⾮常耗时，于是就 回应 A 请求已经处理好的部分，接着回应 B 请求，完成后，再回应 A 请求剩下的部分。

- 服务器推送

  HTTP/2 在一定程度上改善了传统的 **请求—响应** 工作模式，**服务端不再只能被动响应，也可以主动向客户端发送消息**

  举例来说，在浏览器刚请求 HTML 的时候，就提前把可能会⽤到的 JS、CSS ⽂件等静态资源主动发给客户端，减 少延时的等待，也就是服务器推送（Server Push，也叫 Cache Push）



#### HTTP/2 的缺陷？HTTP/3 做了哪些优化？

HTTP/2 主要的问题在于，多个 HTTP 请求复用同一个 TCP 连接，这对于 TCP协议来说是无感的，一旦发生了丢包现象，就会触发 TCP 重传，**这样在一个 TCP 连接中的所有 HTTP 请求都必须等待这个丢了的包被重传回来**

- HTTP/1.1 中的管道网络传输会有队头阻塞的问题
- HTTP/2 多个请求复⽤⼀个TCP连接，⼀旦发⽣丢包，就会阻塞住所有的 HTTP 请求

这是 TCP 传输层的问题，**所以 HTTP/3 就把 TCP协议 改为了 UDP**

UDP 不管顺序，也不管丢包，所以不会出现 HTTP/1.1 的队头阻塞和 HTTP/2 的一个丢包全部重传问题

**UDP 是不可靠传输，但基于 UDP 的 QUIC 协议可以实现类似 TCP 的可靠性传输**

- 当某个流丢包时，只会阻塞这个流，其他流不受影响
- TLS3 升级为了 1.3 版本，头部压缩算法升级为了 `QPACK`
- HTTPS 建立连接，TCP 三次握手，TLS/1.3 三次握手。QUIC 将这六次交互合并成了 3 次，减少了交互次数

所以， QUIC 是⼀个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复⽤的协议



### HTTP/1.1 如何优化？

------

长连接，这个是 HTTP/1.1 的默认选项，从传输层入手，减少无效的 TCP 连接断开重连，来减轻服务器的压力，减少网络传输的延迟

除此之外，还有三个方向：

- 尽量避免发送 HTTP 请求
- 必须发送 HTTP 的请求时，考虑如何减少次数
- 减少服务器的 HTTP 响应的数据大小



#### 如何避免发送 HTTP 请求？

缓存，对于一定时间内可能重复请求，并且数据在一定时间内可能不会改变的请求，就应该缓存到本地（磁盘或内存）

客户端会把第一次请求以及响应的数据保存在本地，URL 为 key，响应作为 value

后续发起相同请求时，就先在缓存中查找该请求的相应数据，找到，直接从本地读取该响应。

缓存下来的数据包含头部信息，其中会有该响应缓存的过期时间，如果发现缓存的数据是过期的，就会重新发送请求。

客户端重新发送请求时，在请求的头部信息中，有一个 `ETag`，它的值是缓存的响应中的摘要（唯一标识响应的资源），服务端收到请求之后，会把这个摘要和服务端资源的摘要进行对比：

- 如果不同，说明客户端的缓存失效了，服务端会在响应中带上最新资源
- 如果相同，说明客户端缓存依旧是最新的，那就返回 `不含 body 的 304 Not Modified 响应`



#### 如何减少 HTTP 请求次数？

- 减少重定向请求次数

  - 每一次重定向就意味着多进行一次 HTTP 请求，这无疑会降低网络的性能

  - 服务端往往不止有一台服务器，比如源服务器上一级是代理服务器，与客户端直接通信的是代理服务器，那每次 请求-响应，都会重复两遍，如下

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url1` 请求
    3. 源服务器 —> 代理服务器，返回 `/url1` 响应（302 Found; Location: /url2）
    4. 代理服务器 —> 客户端，返回 `/url1` 响应（302 Found; Location: /url2）
    5. 客户端 —> 代理服务器，发送 `/url2` 请求
    6. 代理服务器 —> 源服务器，发送 `/url2` 请求
    7. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    8. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

    **如果能让 代理服务器 完成重定向的工作，那就能减少 HTTP 请求的次数**

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url1` 请求
    3. 源服务器 —> 代理服务器，返回 `/url1` 响应（302 Found; Location: /url2）
    4. 代理服务器 —> 源服务器，发送 `/url2` 请求
    5. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    6. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

    **再进一步，如果 代理服务器 不用与 源服务器 沟通，就能知道这个请求要重定向，直接请求重定向 URL，那又减少了 HTTP 请求的次数**

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url2` 请求
    3. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    4. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

- 合并请求

  如果把多个小数据量的请求合并为一个请求，虽然传输资源总量不变，但是请求次数减少，就等同于 **减少了重复发送的 HTTP 头部（目的等同于 HTTP/2 的头部压缩）**

  除此之外，**合并请求也能预防队头阻塞的问题**

  合并请求有以下几种方法：

  1. `CSS Image Sprites` 把多个小图片、小图标合并为一个大图片，一次性请求返回，然后再根据 CSS 数据把大图片切割成多张小图片（减少了 HTTP 请求次数）
  2. 服务端使用 `webpack` 等打包工具把 `JS/CSS` 等资源打包成大文件（减少了 HTTP 请求次数）
  3. 还可以把图片的二进制数据用 `base64` 编码后，以 URL 的形式嵌入到 HTML 文件，跟随 HTML 文件一起发送（这样客户端收到 HTML 后，就可以直接解码出数据，然后直接显示图⽚，就不⽤再发起图⽚相关的请求，这样便减少了请求的次数）

  合并请求就是为了合并资源，以一个大资源的请求替换多个小资源的请求

  **问题：如果其中某个小资源发生了变化，那么整体都要重新请求，这就会带来额外的性能开销**

- 延迟发送请求

  每次都请求用户看到的页面资源，当用户有查看新资源的需求的时候，在再向服务端请求新的资源，这样就做到了延迟发送请求的效果



#### 如何减少 HTTP 响应的数据大小？

- 无损压缩

  > 资源经过压缩后，信息不被破坏，还能完全恢复到压缩前的原样

  适合文本文件、程序可执行文件、程序源代码

  常见的有：gzip，deflate，br（效率比 gzip 高）

  `Accept-Encoding` 请求头就是客户端告诉服务端，客户端支持的压缩算法

  `content-encoding` 响应头就是服务端告诉客户端该资源使用的压缩算法

- 有损压缩

  > 主要将次要的数据舍弃，牺牲⼀些质量来减少数据量、提⾼压缩⽐；解压的数据会与原始数据不同但是⾮常接近

  常用于多媒体数据，比如音频、视频、图片

  可以通过 HTTP 请求头部中的 `Accept` 字段⾥的 `q 质量因⼦`，告诉服务器期望的资源质量

  对于图片的压缩，目前压缩比较高的是 **Google 的 WebP 格式**

  对于音视频的压缩，⾳视频主要是动态的，每个帧都有时序的关系，通常时间连续的帧之间的变化是很⼩的。

  ⽐如，⼀个在看书的视频，画⾯通常只有⼈物的⼿和书桌上的书是会有变化的，⽽其他地⽅通常都是静态的，于是 只需要在⼀个静态的关键帧，使⽤增量数据来表达后续的帧，这样便减少了很多数据，提⾼了⽹络传输的性能。对 于视频常⻅的编码格式有 H264、H265 等，⾳频常⻅的编码格式有 AAC、AC3



### RSA 握手解析

------

HTTPS 是在 HTTP 和 TCP 之间添加了 SSL/TLS 安全传输层，来解决 HTTP 的一些信息安全问题

SSL/TLS 1.2 的连接需要进行四次握手，2 个 RTT（Round-Trip Time，往返时延），SSL/TLS 1.3 对此进行了优化，只需要进行三次握手 也就是 1 个 RTT

HTTPS 进行 SSL/TLS 握手的时候采用的是混合加密模式，建立连接的时候使用非对称加密通信，交换秘钥，然后以相同的加密算法合成出 对称加密使用的 会话秘钥

不同的密钥交换算法，TLS 握手过程会有一些区别

这部分就是通过一个具体的密钥交换算法：RSA，去观察 TLS 握手的一些细节

#### RSA 握手过程

传统 TLS 握手基本都是使用 RSA 算法实现密钥交换的，在将 TLS 协议部署到服务端时，证书文件包含一对公私钥，公钥会在 TLS 握手阶段传递给客户端，私钥服务端使用

RSA 秘钥协商算法中，客户端会生成随机秘钥，并使用服务端的公钥加密后再传给服务端，公钥加密的信息，只能由对应私钥解密

##### TLS 第一次握手

客户端发送一个 **Client Hello** 消息

三部分信息：

- 客户端使用的 SSL/TLS 版本
- **Client Random** 随机数
- 客户端支持的加密方法列表

##### TLS 第二次握手

服务端接收到 **Client Hello** 消息，向客户端发送 **Server Hello** 消息

- 服务端确认支持的 SSL/TLS 版本
- **Server Random** 随机数
- 服务端选择的加密方法

加密方法比如 `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256(0xc02b)`

加密方法的基本形式是 **密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法**

WITH 前面两个单词，第一个是 约定密钥交换的算法，第二个是 约定证书的验证算法（如果只有一个，说明 两个算法都是用这个）

- 握手后的通信使用 AES 对称算法，秘钥长度 128 位，分组模式是 GCM
- 摘要算法 SHA256 用于消息认证和产生随机数

验证服务端身份是通过 **Server Certificate** 消息，其中包含了该服务端的数字证书

然后是 **Server Hello Done** 消息，表示此次应答结束

##### 客户端验证证书

数字证书一般包含如下信息

- 服务端的公钥
- 持有者信息
- CA 的信息
- CA 的数字签名以及使用的算法
- 证书有效期
- 额外信息

数字证书包含两部分内容：

1. 持有者的公钥，用途，颁发者，有效时间等信息，通过 Hash 计算得出的 Hash 值
2. CA 用自己的私钥把 Hash 值加密后，生成的 **数字签名**

客户端校验证书：

1. 客户端使用同样的 Hash 算法获取证书上的 Hash 值 H1
2. 客户端使用 CA 的公钥解密数字签名，获得另一个 Hash 值 H2
3. 比较 H1 和 H2，相同就是有效证书



**证书信任链**

证书一般是有多层的，跨层级的证书之间是没有信任可言的，每一层证书都由它的上一级签发方认证，客户端一般只信任 根证书，根证书信任它的下一级证书，这样就形成了 **证书信任链**，**证书只能由颁发者验证是否可信**

这样做是为了确保根证书的绝对安全性，这样某一层级的证书失效了，那上层的证书可能依旧是可信的



##### TLS 第三次握手

客户端验证完证书之后，就会生成一个新的随机数 **pre-master**，用服务器的公钥加密该随机数，通过 **Change Cipher Key Exchange** 消息传给服务端

服务端收到后，用私钥解密，获得 **pre-master**

至此，双方都得到了 三个随机数 **Client Random, Server Random, pre-master**，然后用 约定好的对称加密算法，生成会话秘钥，用于之后 HTTP 请求的加解密

生成完会话秘钥后，客户端会发送一个 **Change Cipher Spec**，通知服务端开始使用加密方式通信

然后，客户端再发送一个 **Encrypted Handshake Message（Finishd）** 消息，把之前所有发送的数据做个摘要，再用会话秘钥加密一下，是为了让服务端验证加密通信是否可用以及之前的握手信息是否被篡改过

**Change Ciper Spec 消息之前传输的信息，都是明文，之后的都是对称加密的密文**



##### TLS 第四次握手

服务器也是发送 **Change Cipher Spec 和 Encrypted Handshake Message** 消息，如果双方验证加密和解密没问题，那么握手完成，之后使用会话秘钥加解密 HTTP 请求



##### RSA 的缺陷

**不支持前向保密**：前两个随机数是明文传输，第三个随机数是公钥加密的，如果服务端的私钥泄露了，随机数就会被解密

DH 秘钥协商算法解决了这一问题



### ECDHE 握手解析

****

####  离散对数

DH 算法的数学基础

> (a^i) % p = b

- a 和 p 是公共参数，即公开的
- 如果我们知道 i 是多少，很容易算出 b 是多少
- 但是如果我们知道 b 是多少，几乎无法算出 i 是多少



#### DH 算法

假设 A 和 B 使用 DH 算法交换秘钥，那么基于离散对数，底数和模数是公开的，我们用 P 和 G 来代称

然后 A 和 B 各自生成自己的 私钥，A 的私钥记为 a，B 的私钥记为 b

现在二者都有 P 和 G，以及各自的私钥，那就可以计算出各自的公钥：

- A 的公钥记为 **a`**，**a`** = (G^a) % P
- B 的公钥记为 **b`**，**b`** = (G^b) % P

双方交换公钥，A 手上有五个参数，a, **a`**, G, P, **b`**；B 手上也有五个参数，b, **b`**, G, P, **a`**

因为

(**a`** ^ b) % P = (((G ^ a) % p) ^ b) % P = (G ^ (a * b)) % P

(**b`** ^ a) % P = (((G ^ b) % P) ^ a) % P = (G ^ (a * b)) % P

所以 A 和 B 双方可以使用自己手中的参数，算出一个相同的数，我们称之为 K

K 就可以用来当做 **对称加密秘钥**

其中 G，P，**a`**，**b`** 是公开的，无法破译出 a 或者 b 就无法算出 对称加密秘钥，虽然理论上可以通过离散对数的逆运算算出 a 或者 b，但是几乎不可能，因此 DH 算法是安全的



#### DHE 算法

根据私钥生成的方式，DH 算法分为两种具体实现：

- static DH 算法，已废弃

  static DH 算法中，一方（通常是服务端）的 私钥是不变的，另一方的私钥是 每次 TLS 连接实时生成的。

  私钥固定，意味着公钥也是不变的，有了大量的秘钥协商过程的数据之后，就可以暴力破解出固定的私钥了，所以 static DH 算法不具备前向安全性

- DHE 算法，现在常用

  DHE 算法就是双方的私钥每次 TLS 连接都是动态生成的，那即使某一次连接的私钥被破解了，其他的通信过程依然是安全的。**每个通信过程 的私钥都是没有任何关系的，都是独⽴的，这样就保证了前向安全**



#### ECDHE 算法

DHE 算法由于计算量较大，所以性能欠佳，ECDHE 解决了 DHE 的性能问题

简单来说，就是优化了 公钥的计算过程

- 双⽅事先确定好使⽤哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；
- 双⽅各⾃随机⽣成⼀个随机数作为私钥d，并与基点 G相乘得到公钥Q（Q = dG），此时⼩红的公私钥为 Q1 和 d1，⼩明的公私钥为 Q2 和 d2；
- 双⽅交换各⾃的公钥，最后⼩红计算点（x1，y1） = d1Q2，⼩明计算点（x2，y2） = d2Q1，由于椭圆曲线 上是可以满⾜乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 ，因此双⽅的 x 坐标是⼀样的，所以 它是共享密钥，也就是会话密钥。

#### ECDHE 握手过程

**RSA 必须完成 四次握手才能传输应用数据** 

**ECDHE 在第四次握手之前，客户端就已经发送了加密的 HTTP 数据**，这样提高了传输的效率



##### TLS 第一次握手

客户端发送 **Client Hello** 消息，包干如下信息

- 客户端使用的 SSL/TLS 版本号
- 客户端生成的随机数 **Client Random**
- 客户端支持的加密套件列表



##### TLS 第二次握手

服务端发送 **Server Hello** 消息，包含如下信息

- 服务端确认的 SSL/TLS 版本号
- 服务端生成的随机数 **Server Random**
- 服务端选择的加密套件

加密套件比如：`TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384`

- 秘钥协商算法使用 ECDHE
- 签名算法使用 RSA
- 握手后的通信使用 AES 对称算法，秘钥长度 256 位，分组模式是 GCM
- 摘要算法使用 SHA384

然后服务端发送 **Certificate** 消息，包含服务端的数字证书

然后就是与 RSA 握手过程不同的了，**服务端发送 Server Key Exchange** 消息，做了下面几件事

- 选择 椭圆曲线，目的是确定 **基点 G**
- **生成随机数作为服务端的私钥**，保留在本地
- 根据 基点 G 和 私钥计算出**服务端的椭圆曲线公钥**

选择的椭圆曲线 和 服务端的椭圆曲线公钥会随着消息发送给客户端

为了保证这个椭圆曲线的公钥不被第三⽅篡改，**服务端会⽤ RSA 签名算法给服务端的椭圆曲线公钥做个签名**

然后就是 **Server Hello Done** 消息



##### TLS 第三次握手

首先还是验证数字证书是否合法，走证书信任链，逐级验证证书，确认服务端身份

然后**客户端生成随机数作为客户端的私钥**，再根据 基点 G 和私钥 **计算出客户端的椭圆曲线公钥**，使用 **Client Key Exchange** 消息把客户端公钥发送给服务端

然后双方就都持有自己的私钥，对方的公钥，椭圆曲线基点 G，然后就可以计算出点坐标，其中 x 坐标双方相同，理论上 **x 已经可以作为 对称加密的秘钥了**，但是实际中，需要使用 **Client Random, Server Random, x** 三者使用 对称算法，算出最终的会话秘钥

因为计算机中不存在真正的随机数，都是伪随机，三个伪随机数合成一个随机数，更随机一些

然后客户端发送 **Change Cipher Spec** 消息，通知服务端之后使用对称加密通信

**Encrypted Handshake Message** 消息，把之前的数据做个摘要，再用对称秘钥加密，发送给服务端做验证



##### TLS 第四次握手

最后，服务端也会有⼀个同样的操作，发 

Change Cipher Spec 和 Encrypted Handshake Message 消息，如果双⽅都验证加密和解密没问题，那么握⼿正式完成。于是，就可以正常收发加密的 HTTP 请求和响应了



##### 总结

- RSA 秘钥协商算法不支持 前向保密，ECDHE 算法支持
- RSA 算法必须等待四次握手结束才能进行数据传输；ECDHE 算法在第四次握手之前就可以了，节省了时间
- ECDHE 算法在 TLS 第二次握手中会发送 Server Key Exchange 消息，RSA 算法没有



### HTTPS 如何优化

****

#### 性能损耗分析

两方面

- TLS 协议握手过程
- 对称加密报文传输

第二个过程来说，主流的对称加密算法 AES，chacha20 性能都很好，有些 CPU 也做了硬件级别的优化

所以主要的优化目标是 TLS 协议握手过程

- 对于 ECDHE 算法，TLS 握手过程中，会动态生成双方的公私钥
- 客户端验证数字证书时，也会访问 CA
- 计算对称加密秘钥



#### 硬件优化

**HTTPS 协议是计算密集型，不是 I/O 密集型，需要更加强大的计算能力（CPU）**

可以选择支持 **AES-NI 特性的 CPU，它优化了 AES 算法**



#### 软件优化

主要就是软件升级



#### 协议优化

- 密钥交换算法优化

  RSA 算法必须进行 四次握手（TLS 1.2），而且不具备前向安全性

  ECDHE 算法可以在 三次握手之后就进行 HTTP 对称加密信息传输，由 2 RTT 减少到了 1 RTT，具备前向安全性

  不同的椭圆曲线性能也不一样，目前最快的是 **x25519 曲线**

  握手之后的对称加密算法中，**AES_128_GCM 比 AES_256_GCM** 快一些，安全性差一些

- TLS 升级

  TLS 1.3 只需要进行 3 次握手，1 RTT

  1.2 中，客户端向服务端发送 公钥是在 第三次握手；1.3 优化的就是这个过程，直接把 第三次握手的动作放入到了第一次握手中

  具体操作就是：客户端在 Client Hello 消息中带上了客户端支持的椭圆曲线，以及这些椭圆曲线对应的公钥

  服务端接收到之后，选定一个椭圆曲线，然后把对应的公钥返回给客户端，这样，1 RTT 内，双方就都有了生成对称加密秘钥的全部信息了

  **1.3 不在支持 RSA 和 DH 密钥交换算法，只支持 ECDHE 算法**



#### 证书优化

- 证书传输

  对于服务 器的证书应该选择椭圆曲线（ECDSA）证书，⽽不是 RSA 证书，因为在相同安全强度下， ECC 密钥⻓度⽐ RSA 短的多

- 证书验证

  客户端在验证证书时，是个复杂的过程，会⾛证书链逐级验证，验证的过程不仅需要「⽤ CA 公钥解密证书」以及 「⽤签名算法验证证书的完整性」，⽽且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性

  这个访问过程是 HTTP 访问，因此⼜会产⽣⼀系列⽹络通信的开销，如 DNS 查询、建⽴连接、收发数据等

  > **CRL**：证书吊销列表，这个列表是由 CA 定期更新，列表内容都是被撤销信任的 证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的

  CRL 有两个问题：

  - 实时性差
  - 随着吊销证书的增多，列表会越来越⼤，下载的速度就会越慢

  > **OCSP**：CRL 的替代品，名为在线证书状态协议，来查询证书的有效性，它的⼯作⽅式是向 CA 发送查询请求，让 CA 返回证书的有效状态

  OCSP Stapling

  为了解决 OCSP 还需要向 CA 发送请求这一网络开销

  > **OCSP Stapling**：服务器向 CA 周期性地查询证书状态，获得 ⼀个带有时间戳和签名的响应结果并缓存它。当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握⼿过程中发给客户端。由于有签名的存在， 服务器⽆法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询



#### 会话复用

TLS 握手是为了协商对称加密秘钥，如果把首次 TLS 协商的对称加密秘钥缓存起来，下次需要建立 HTTPS 连接直接复用，就可以减少性能损耗，这就是会话复用，分两种：

- Session ID

  客户端和服务器⾸次 TLS 握⼿连接后，双⽅会在内存缓存会话密钥，并⽤唯⼀的 Session ID 来标识，Session ID 和会话密钥相当于 key-value 的关系

  客户端再次连接时，hello 消息中会带上 Session ID，服务器收到后就会从内存找，找到就直接恢复，只用 1 个 RTT，但是会定期失效

  缺点：

  - 服务器的内存压力会越来越大
  - 多台服务器负载均衡，客户端再次连接不⼀定会命中上次访问过的服务 器，于是还要⾛完整的 TLS 握⼿过程

- Session Ticket

  服务器不再缓存每个客户端的会话密钥，⽽是把缓存的⼯ 作交给了客户端，类似于 HTTP 的 Cookie

  首次建立 TLS 连接后，服务器会加密 会话密钥 作为 Ticket 发给客户端，交给客户端缓存该 Ticket

  客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上⼀次的会话密钥，然后验证有效期， 如果没问题，就可以恢复会话了，开始加密通信

**Session ID 和 Session Ticket 都不具备前向安全性**

- Pre-shared Key

  TLS 1.3 会话复用的方式，重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这样只需要 0 RTT 就能重连



## TCP

------

### TCP 基本认识

****

#### TCP 头格式

重要字段有以下几个：

- 序列号：通过 SYN 包传递给接收端主机，**用来解决网络包乱序问题**
- 确认应答号：下一次 **期望** 收到的数据的序列号，发送端收到这个，就默认这个序列号之前的数据都被正常接收了，**用来解决不丢包问题**
- 控制位：
  - `ACK`：**确认应答字段生效**，TCP 规定除了最初建立连接时的 SYN 包之外，该位必须设置为 1
  - `RST`：TCP 连接出现异常，必须强制断开连接
  - `SYN`：希望建立连接，并在其 序列号 字段进行序列号初始值的设定
  - `FIN`：希望断开连接



#### 为什么需要 TCP 协议？TCP工作在哪一层？

因为 IP 协议 不能保证网络包的交付，网络包的有序交付，也不能保证网络包中数据的完整性

TCP 协议是工作在 **传输层** 的 **可靠的** 数据传输服务，他能确保接收端接收的网络报是 **无损坏、无间隔、非冗余、按序的**



#### 什么是 TCP？

> TCP 是 **面向连接的、可靠地、基于字节流的传输层通信协议**

- 面向连接：必须是 **一对一连接**，不能像 UDP 一样 **一个主机同时向多个主机发送消息**
- 可靠的：无论网络链路出现了怎样的链路变化，TCP 都可以保证一个报文一定能到达接收端
- 字节流：消息是 没有边界的，所以无论多大都可以传输。消息也是有序的，前一个消息没有收到，即便先收到后一个消息，应用层也不能处理，并且重复的报文会自动丢弃



#### 什么是 TCP 连接？

> 用于保证可靠性和流浪控制维护的某些状态信息，这些信息的组合，包括 socket、序列号和窗口大小，成为连接

- Socket：由 IP 地址和端口号组成
- 序列号：用来解决乱序等问题
- 窗口大小：控制流量



#### 如何唯一确定一个 TCP 连接？

TCP 四元组

- 源地址
- 源端口
- 目的地址
- 目的端口

源地址和目的地址的字段（32位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机

源端口和目的端口的字段（16位）在 TCP 头部中，作用是告诉 TCP 协议应该把报文发送给哪个进程



#### 有一个 IP 的服务器监听了一个端口，它的 TCP 最大连接数是多少？

服务端的一个接口可以给多个客户端提供服务

理论上讲：最大 TCP 连接数 = 客户端的 IP 数 * 客户端的端口数

IPV4，客户端的 IP 数最多为 2^32，客户端的端口数最多为 2^16

也就是说，**理论上讲，服务器单机的最大 TCP 连接数约为 2^48**

实际上，服务端最大并发 TCP 连接数远不能达到理论上限

- **文件描述符** 限制：Socket 都是文件，所以首先要通过 `ulimit` 配置未见描述符的数目
- **内存限制**：每个 TCP 连接都要占用一定的内存，操作系统的内存是有限的



#### 什么是 UDP？

UDP 不提供复杂的控制机制，利用 IP 提供面向 **无连接** 的通信服务

UPD 的头部格式如下：

- 源端口号：(2 字节)
- 目标端口号：(2 字节) 源端口和目标端口主要是 告诉 UDP 协议该把报文发送给哪个进程
- 包长度：(2 字节) 该字段保存了 UDP 首部和数据的长度之和
- 校验和：(2 字节) 为了提供可靠地 UDP 首部和数据



#### UDP 和 TCP 有什么区别？

| 比较点   | TCP                                     | UDP                                                      |
| -------- | --------------------------------------- | -------------------------------------------------------- |
| 是否连接 | 面向连接                                | 无连接                                                   |
| 连接对象 | 一对一                                  | 一对一、一对多、多对多                                   |
| 是否可靠 | 可靠                                    | 不可靠                                                   |
| 头部开销 | 20~60 字节                              | 8 字节                                                   |
| 传输方式 | 面向字节流（会在 传输层 根据 MSS 分片） | 面向报文（会在 IP 层 根据 MTU 分片，这是 IP 协议的功能） |
| 传输速度 | 由流量控制和拥塞控制调整                | 有数据就发，很快                                         |



#### UDP 和 TCP 的应用场景分别是什么？

- TCP 是面向连接的，能保证数据的 **可靠性** 交付
  - `FTP` 文件传输
  - `HTTP` /  `HTTPS`
- UDP 面向无连接，可以**随时发送数据**，UDP 本身的处理**简单高效**
  - 包总量较少的通信，如 `DNS`、`SNMP` 等
  - 视频、音频等多媒体通信
  - 广播通信

| 应用层协议 | 应用     | 传输层协议 |
| ---------- | -------- | ---------- |
| SMTP       | 电子邮件 | TCP        |
| HTTP       | 万维网   | TCP        |
| FTP        | 文件传输 | TCP        |
| DNS        | 域名解析 | UDP        |
| TFTP       | 文件传输 | UDP        |
| SNTP       | 网络管理 | UDP        |



#### 为什么 UDP 头部没有 首部长度 字段，而 TCP 头部有

原因是 TCP 的头部长度是可变的，至少 20 字节；UDP 的头部长度是固定的 8 字节



#### 为什么 UDP 头部有 包长度 字段，而 TCP 头部没有

TCP 数据长度 = IP 总长度 - IP 首部长度 - TCP 首部长度

IP 总长度 和 IP 首部长度是在 IP 首部中知道的；TCP 的 首部长度 是在 TCP 首部中知道的。所以就可以求得 TCP 数据的长度了

按理来说，UDP 也是不需要 包长度 字段的

但是，**为了⽹络设备硬件设计和处理⽅便，⾸部⻓度需要是 44 字节的整数倍**

如果去掉 UDP 包长度 字段，UDP 的首部长度就不是 4 字节的整数倍了



### TCP 连接建立

****

#### TCP 三次握手过程和状态变迁

- 一开始，客户端和服务端都处于 `CLOSED` 状态，先由服务端监听某个端口，服务端变更为 `LISTEN` 状态
- **三次握手的第一个报文**：**客户端会随机初始化序列号（`client_isn`），置于 TCP 首部的 序列号字段，同时 把 `SYN` 标志位置为 1**，表示这是一个 `SYN` 报文，然后发送给服务端，向服务端请求建立连接，**SYN 报文不包含应用层数据**，然后 **客户端变更为 `SYN-SENT` 状态**
- **三次握手的第二个报文**：服务端接收到 `SYN` 报文， **服务端也会初始化序列号（server_isn），置于 TCP 首部的 序列号字段，把 `client-isn + 1` 填入 确认应答号 字段，同时把 SYN 和 ACK 标志位置为 1**，表示 这是一个 `SYN + ACK` 报文，然后发送给客户端，**该报文也不包含 应用层数据，之后服务端变更为 `SYN-RCVD` 状态**
- **三次握手的第三个报文**：客户端收到 `SYN-ACK` 报文后，还要向服务端发送最后一个报文。**把 `server-isn + 1` 填入 确认应答号 字段，把 `ACK` 标志位置为 1**，最后发送给服务端，**这次报文可以携带 客户端的数据，之后客户端状态变更为 `ESTABLISHED`**
- 服务端收到客户端的应答报文后，**也把状态变更为 ESTABLISHED**

**前两次握手不能携带应用层数据，第三次可以**



#### 为什么是三次握手？不是两次、四次？

TCP 是 **面向连接的**，需要维护 Socket、序列号、窗口大小这些信息，以建立连接

以下是三次握手的原因：

- 三次握手才可以阻止重复历史连接的初始化（主要原因）

  - **三次握手的首要原因就是防止旧的重复连接初始化造成混乱**

  场景：

  客户端连续发送多次 SYN 建⽴连接的报⽂，在⽹络拥堵情况下：

  - 一个 **旧 SYN 报文** 比 **新 SYN 报文** 先到达服务端
  - 然后服务端就会响应一个 **SYN + ACK 报文（对应 旧 SYN 报文）**
  - 客户端收到后，根据自身上下文判断这是一个历史连接（我猜可能是用 确认应答号 字段 判断的），**客户端就会发送 `RST` 报文给服务端，表示终止这次连接**

  **如果两次握手，就没有 判断是否是历史连接这一操作的时间，三次握手则可以在客户端准备发送第三次报文的时候，判断当前连接是否是历史连接**

  - 是历史连接，第三次握手就发送 `RST` 报文，表示终止连接
  - 不是历史连接，发送 `ACK` 报文，表示建立 TCP 连接成功

- 三次握手才可以同步双方的初始序列号

  序列号的作用：

  - 接收方可以去除重复的数据
  - 接收方可以根据数据包的序列号按序接收
  - 可以标识发送出去的数据包中，哪些是成功被对方收到的

  TCP 协议的通信双方，都必须维护一个自己的 序列号，以保证数据的可靠传输

  **一来一回，才能确保双方的初始序列号能同步**

  所以，理论上来讲：

  - 第一个来回，客户端发送自己的初始序列号，给服务端同步，服务端发送 确认应答号，表示服务端已同步客户端的初始序列号
  - 第二个来回，服务端发送自己的初始序列号，给客户端同步，客户端发送确认应答号，表示客户端已同步服务端的初始序列号

  这样的话，应该是四次握手，**但是服务端发送确认应答号和初始序列号两步可以优化成一步，所以就成了三次握手**

  **两次握手只能确保同步一方的初始序列号**

- 三次握手才可以避免浪费资源

  如果 只有  两次握手，服务端每接收到一个 `SYN` 报文，就必须建立一个连接，**因为服务端不知道客户端是否收到了自己的 `ACK` 确认建立连接报文**

  这样的话，一旦客户端发送的 `SYN` 报文在网络中阻塞了，客户单就会重复发送多次 `SYN` 报文，服务端就会建立多个 TCP 连接，但这些都是冗余的，会造成资源浪费

总结：

**TCP 为什么是三次握手：是因为三次握手能防止历史连接的初始化；同步连接双方的初始序列号；减少不必要的资源浪费**

**两次握手不能做到以上三点；四次握手不需要，因为三次足够了**



#### 为什么初始序列号要求每次连接随机生成？

为了确认是历史连接的报文，还是当前连接的报文，防止接收数据紊乱；防止黑客伪造相同序列号的 TCP 报文被对方接收



#### 初始序列号 是如何随机产生的？

ISN = M + F

- M 是一个计时器，每隔 4 ms 加一
- F 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口随机生成



#### IP 层会分片，为什么 TCP 还需要 MSS？

网络包由以下几部分组成：

1. 报头/起始帧分解符
2. MAC头部
3. IP头部
4. TCP头部
5. 数据
6. FCS

其中 MTU 就是 3/4/5；MSS 就是 5

- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节
- `MSS`：除去 IP 和 TCP 头部，一个网络包所能容纳的 TCP 数据的最大长度

IP 层没有超时重传机制，传输层的 TCP 负责超时和重传

**只用 IP 分片的话，一个分片丢失，整个 TCP 报文都要重传。**当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，就不会响应 ACK 给对方，发送方的 TCP 超时后，就会 **重发整个 TCP 报文（头部 + 数据）。**这样是很低效的

所以，TCP 协议在 **建立连接的时候（TCP 第一次握手，SYN 报文）通常会协商MSS 值**，当超过这个大小的时候，**TCP 报文就会在 传输层分片，这个 IP 包的长度必然小于 MTU，所以就不会在 IP 层分片**，这样，**如果某个 TCP 分片丢失，那只需要重传这一个 MSS 的数据，增加了重传的效率**



#### 什么是 SYN 攻击？

TCP 建立连接要三次握手，假如攻击者短时间内伪造不同 IP 地址发送 `SYN` 报文，服务端每接收一个 `SYN` 报文，就会进入 `SYN_RCVD` 状态，但是服务端发送出去的 `SYN + ACK` 报文，无法得到客户端的 `ACK` 应答，过多这样的建立连接请求就会占满服务端的 `SYN` 接收队列（半连接队列），服务器就不能正常工作了



### TCP 连接断开

****

#### TCP四次挥手过程和状态变迁

TCP 连接 **双方都可以主动断开连接**，断开连接后，主机中的资源会被释放

> MSL（Maximum Segment Lifetime）最大报文生存时间：它是任何报文段被丢弃前在网络内的最长时间。RFC 793指出MSL为2分钟，现实中常用30秒或1分钟

- 客户端打算关闭连接，会发送一个 `FIN` 报文，`FIN` 标志位会置为 1，客户端变更为 `FIN_WAIT_1` 状态
- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，服务端变更为 `CLOSED_WAIT` 状态
- 客户端接收到服务端的 `ACK` 应答报文后，变更为 `FIN_WAIT_2` 状态
- 服务端处理完数据后，也会向客户端发送 `FIN` 报文，服务端变更为 `LAST_ACK` 状态
- 客户端收到服务端的 `FIN` 报文后，回应一个 `ACK` 报文，客户端变更为 `TIME_WAIT` 状态
- 服务端收到 `ACK` 报文后，状态变更为 `CLOSED`，服务端完成连接的关闭
- 客户端在经过 `2MSL` 后，自动变更为 `CLOSED` 状态

**每个方向都需要一个 FIN 和 一个 ACK，因此通常称为 四次挥手**

**主动关闭连接的，才有 `TIME_WAIT` 状态**



#### 为什么挥手要四次？

假设 A 方要关闭，B 方被关闭

- A方先发送 `FIN` 报文，表示 **我不再发送数据了，但是可以接收**
- B方接收到 `FIN` 报文后，回应一个 `ACK` 报文，**但是 B 方可能还有数据要处理，发送给 A 方**，等 B 方不再需要发送数据给 A 方了，就发送一个 `FIN` 报文，**表示同意关闭连接**

这样来看，被动的一方通常可能会有数据需要再发送给主动的一方，所以 `ACK` 和 `FIN` 报文通常会分开发送，所以比三次握手多了一次



#### 为什么 `TIME_WAIT` 等待的时间是 2 MSL

> TTL：是 IP 数据报可以经过的最⼤路由数，每经过⼀个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报⽂通知源主机

- 防止具有相同 **四元组** 的 旧 数据包被下个连接接收到

  `2MSL` 的时间，足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都消失，再出现的数据包一定是新连接的

- 保证 **被动关闭方** 能正确的关闭，即 保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭

  等待足够长的时间，以确保最后的 ACK 能让 被动方接收，正常关闭

  如果 ACK 报文丢失了，但是 TIME_WAIT 状态已经结束了，那主动方正常关闭了，被动方却还在等 ACK 报文，那被动方就会一直处于 LAST_ACK 状态

  `2MSL` 是从 ACK 发出开始计时的，完全足够 ACK 到达被动方，或者被动方没收到，重传 `FIN` 报文，这样肯定能保证 被动方也正常关闭



### TCP 重传

****

TCP 连接中，发送端发出一条消息后，接收端正常接收到的话会回复一个确认应答消息，表示已收到，这个机制是依靠序列号实现的

如果数据在传输过程中丢失了的话， TCP 就会用重传机制解决

常见的重传机制有：

- 超时重传
- 快速重传
- SACK
- D-SACK

#### 超时重传

发送数据时，设定一个定时器，超过指定时间后，如果没有收到接收端的 `ACK（确认应答）` 报文，就会触发 **超时重传**

两种情况会触发超时重传：

- 数据包丢失
- 确认应答丢失

##### 超时时间设置为多少合适？

超时重传时间是以 `RTO` （Retransmission Timeout 超时重传时间）表示的

- `RTO` 较大：如果 RTO 太大了，重发间隔就会很大，那样性能就会很差
- `RTO` 较小：如果 RTO 太小的话，可能导致没有丢包就触发超时重传了，性能也有影响

所以，`RTO` 不能太大，也不能太小，**边界就是 `RTT`，`RTO` 应当略微大于 RTT，这样既不会在没有丢包的时候触发超时重传，也不会因为重传间隔太大而导致性能很差**

**因为 RTT 是波动变化的，所以 RTO 也应该是动态变化的**

每当遇到⼀次超时重传的时候，都会将下⼀次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送



#### 快速重传

快速重传不是根据时间判断是否重传的，**而是根据数据**

简单来说，如果发送方某个数据包丢失，即使它继续发送之后的数据包，接收方的确认应答报文中的 **确认应答号** 依旧是未接收到的第一个数据包，**连续三次收到相同的 确认应答报文后，就会触发快速重传**

快速重传就是为了解决超时时间的问题，但是快速重传也有问题，**重传的时候，是重传之前的一个，还是重传所有？因为发送端并不知道三个相同的确认应答报文是哪个数据包传回来的**



#### SACK 方法

为了解决不知道该重传哪些 TCP 报文，就实现了 `SACK` 方法（ `Selective Acknowledgment` 选择性确认）

实现手段就是：**在 TCP 头部的 选项 字段中加一个 `SACK`，用来存储缓存地图，发送方就可以知道哪些数据没有收到了**

**发送方连续三次收到同样的 ACK 确认报文，就会触发快速重传机制，`SACK` 信息帮助发送端确认丢失的数据，只重发丢失的数据**

**如果要支持 `SACK`，必须双方都支持**



#### Dumplicate SACK

又称 `D-SACK`，主要使用 `SACK` 哪些数据被重复接收了

场景：

- `ACK` 丢包

  如果触发重传是因为 `ACK `应答报文丢失而触发 **超时重传机制**，那接收方就会收到重复的数据，再次发送的 `ACK` 应答报文中的 `SACK` 就会记录重复收到的记录

- 网络延时

  当因为网络延时而迟迟未到达接收方的数据触发 **快速重传机制**，而重新发送过去，然后这个慢的报文才到达接收方的时候，就会知道是因为 网络延时才触发的快速重传

好处：

- 可以知道是发出去的包丢失了，还是 `ACK` 包丢失了
- 可以知道是不是网络延迟导致数据包迟迟未到达接收方
- 可以知道网络中是不是把数据包复制了



### 滑动窗口

如果每发送一条数据都要进行一次确认应答，必须这样顺序执行的话，通信效率是比较低的（**RTT 越长，通信效率越低**）

为了解决这个，就有了窗口（RTT 较长也不会影响通信效率）

> 窗口大小：无需等待应答，而可以继续发送数据的最大值

> 窗口：窗口实际上是接收方开辟的一块缓存空间，只要窗口还没满，发送方就可以无需等待应答，继续发送数据

假设窗口大小设置为 3 个 TCP 段，那么发送端就可以连续发送 3 个 TCP 报文，**如果中途有 `ACK` 丢失，可以通过下一个确认应答进行确认**



##### 窗口大小由哪方决定？

TCP 头里有一个 `Window（窗口大小）` 字段

这个字段是接收端告诉发送端⾃⼰还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，⽽不会导致接收端处理不过来



- 发送方窗口

  发送方窗口分为四部分：

  - 已发送并收到 `ACK` 确认的数据
  - 已发送但未收到 `ACK` 确认的数据
  - 未发送但总大小在接收方窗口内的数据
  - 未发送但总大小超过了接收方窗口大小的数据

  TCP 滑动窗口方案用三个指针来跟踪四个类别的连接处：

  - `SND.WND`：发送窗口的大小
  - `SND.UNA`：绝对指针，指向的是已发送但未收到确认的第⼀个字节的序列号
  - `SND.NXT`：绝对指针，指向未发送但可发送范围的第⼀个字节的序列号
  - 指向 未发送但总大小超过了接收方窗口大小范围的第一个字节的 是相对指针，由 `SND.UNA` + `SND.WND` 计算得出

- 接收方窗口

  分为三个部分：

  - 已成功接收并确认的数据
  - 未收到数据但在接收方窗口大小范围内的数据
  - 未收到数据而且超出接收方窗口大小范围的数据

  三个部分，两个指针划分：

  - `RCV.WND`：接收窗口大小
  - `RCV.NXT`：指向第二部分的第一个字节
  - 指向第三部分的第一个字节的是相对指针，由 `RCV.NXT` + `RCV.WND` 计算得出

- 二者大小并不完全相等



### 流量控制

发送方不能一直不间断地发数据给接收方，要考虑接收方的处理能力

如果接收方处理不过来，就会不发送 `ACK ` 确认应答报文，这样就会触发重传机制，造成网络流量的浪费

**TCP 提供了一种机制，可以让 发送方 根据 接收方 的接收能力控制发送的数据量，这就是流量控制**

流量控制是用滑动窗口实现的



#### 操作系统缓冲区 和 滑动窗口 的关系

实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，**大小会被操作系统调整**

场景如下，应用程序没有及时读取缓存：

- 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 `360`
- 服务端非常繁忙，收到客户端的数据时，应用层不能及时读取数据

1. 客户端发送 140 字节的数据，发送方窗口变为 220（360 - 140）
2. 服务端收到 140 字节数据后，由于**非常繁忙，只处理了 40 字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260（360 - 100）**，最后发送确认信息时，140 字节是个整体，肯定全部确认收到，确认应答号就变为了  140 字节的下一个字节的序列号；窗口大小也放入 TCP 头部，通知给发送方
3. 客户端收到 `ACK` 报文后，将发送窗口也减小为 260
4. 客户端发送 180 字节数据，`SND.NXT` 向后移 180 字节，可用发送窗口还有 80 字节
5. 服务端收到 180 字节数据，**但是应用程序没有读取任何数据，这 180 字节直接留在了缓冲区，于是接收窗口继续收缩，变为 80（260 - 180）**，发送 `ACK` 确认报文，这 180 字节数据确认收到，窗口大小变为 80
6. 客户端收到 `ACK` 报文后，发送窗口减小为 80
7. 客户端发送 80 字节数据，发送端可用窗口耗尽
8. 服务端收到 80 字节数据，**应用程序仍旧没有读取数据，80字节留在缓冲区，接收窗口收缩为了 0（80 - 80）**，发送 `ACK` 确认报文，窗口大小 变为 0
9. 客户端收到 `ACK` 报文后，发送窗口减小为 0

**窗口收缩为 0，即窗口关闭，当发送方可用窗口变为 0 时，发送方会定时发送 窗口探测报文，以便知晓接收方的窗口是否发生了变化**



第二个场景，操作系统资源紧张，缓冲区可能会直接减小，如果此时应用程序无法及时读取数据，可能会丢失数据包：

1. 客户端发送 140 字节的数据，发送方窗口变为 220（360 - 140）
2. **服务端繁忙，操作系统就把接收缓存减少了 120 字节，现在接收窗口大小为 240 字节。收到 140 字节数据后，因为应用程序没有读取，所以就留在了缓冲区，接收窗口又减小，变为 100（240 - 140）**然后发送 `ACK` 确认报文，窗口大小
3. 此时 **客户端不知道服务端接收窗口收缩了，发送窗口还有 220 字节，所以客户端又发送了 180 字节**，接收端可用窗口收缩到了 40（220 - 180）
4. 服务端收到 180 字节数据，但是 超过了 接收窗口的大小，所以数据包就丢失了
5. 客户端收到 第二步 发送的 `ACK` 报文的时候，尝试缩小发送窗口为 100，但是尝试之后，可用窗口 40 - 120 = -80，变成了负数，这就意味着已经有超过接收窗口 80 字节的数据被发送出去了，上一个包会被接收方丢弃

**为了防止这种情况发生，TCP 规定不能同时减少缓存又收缩窗口，而是采用先收缩窗口，过段时间再减少缓存**



#### 窗口关闭

TCP 通过让 接收方 告知 发送方 自己的接受能力，来控制窗口大小，从而实现流量控制的

**如果窗口大小收缩为 0，就会阻止发送方给接收方发送数据，直到窗口大于 0 为止，这就是窗口关闭**



##### 窗口关闭潜在的死锁风险

接收方通过 `ACK` 确认应答报文 来告知 发送方 窗口大小

当窗口关闭时，接收方处理完数据了，会向发送方 回应一个 窗口大小 不为 0 的 `ACK` 报文，如果这个报文丢失了，**那么发送方就会一直等待 这个 非 0 报文；接收方也一直等待 发送方 发送数据，这就造成了死锁**



##### TCP 如何解决这个死锁风险的

TCP 为每个链接设有一个持续定时器，**只要 TCP 连接一方收到对方的零窗口通知，就会启动持续定时器**

如果持续定时器超时，就会发送 **窗口探测（Window Probe）报文**，而对方在确认这个探测报文时，给出自己现在的接收窗口大小：

- 如果接收窗口 依旧是 0，那么发送方会重新启动持续计时器
- 如果不是 0，就可以继续传输数据了

**窗口探测的次数一般为 3 次，每次 30 ~ 60 秒，三次后依旧是 0 的话，有的 TCP 实现就会发送 `RST` 报文中断连接**



#### 糊涂窗口综合症

如果接收方一直不读取缓存中的数据，那么窗口会越来越小

**最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，发送方就会立即发送这几个字节，这就是糊涂窗口综合症**

**TCP + IP** 头部至少 40 字节，为了传输几个字节数据，开销太大

要解决 糊涂窗口综合症，就得解决两个问题：

- 接收方不回复小窗口给发送方

  当窗口大小小于 MSS 和 缓存大小的 1/2 中的最小值时，就会发送窗口大小为 0

- 发送方避免发送小数据

  使用 Nagle 算法，**延时处理**，满足以下条件之一才可以发送数据：

  - 等到窗口大小 >= MSS 或者 数据大小 >= MSS
  - 收到之前发送数据的 `ACK` 报文

  Nagle 算法默认开启，对于需要小数据包交互的程序，比如 `telnet、ssh` 这种，就要关闭



### 拥塞控制

#### 为什么要有拥塞控制

**流量控制是为了避免发送方填满接收方的缓存**

数据传输是要走网络的，大家都在通信就有可能使网络拥堵

**网络拥堵时，如果继续发送大量数据包，可能会导致数据包时延，丢失等，这时 TCP 就会触发重传机制，但是重传会加重网络的负担，于是导致更大的延迟更多的丢包，恶性循环**

**拥塞控制的目的就是避免发送方的数据填满整个网络**



#### 拥塞窗口

为了 **调节发送方所要发送的量**，定义了 拥塞窗口

拥塞窗口 cwnd 是 发送方 维护的一个状态量，根据 **网络的拥塞程度动态变化**

发送窗口 swnd 和 接收窗口 rwnd 是约等的关系，现在加入了 cwnd

发送窗口 swnd 变为 接收窗口 和 拥塞窗口的最小值

拥塞窗口 变化规则：

- 只要网络中没有出现拥塞，cwnd 就变大
- 出现拥塞，cwnd 就变小



#### 拥塞窗口有哪些控制算法？

- 慢启动
- 拥塞避免
- 拥塞发生
- 拥塞恢复



##### 慢启动

TCP 建立连接之后，会 **一点一点 增多 发送数据包的 数量，这就是慢启动**，如果一上来就很大，就容易造成阻塞

**发送方没收到一个 ACK，拥塞窗口就加倍**

场景，假定 cwnd 和 swnd 大小相等：

- 一开始初始化 `cwnd = 1`，表示可以传输 一个 `MSS` 大小的数据
- 收到 `ACK` 后，cwnd 加倍，所以一次能够发送 2 个
- 收到 2 个的 `ACK` 应答后，加倍，一次发送 4 个

**慢启动算法，发包个数是指数性增长的**

`ssthresh`（slow start threshold）慢启动门限 状态变量，控制 慢启动增长个数

- 当 cwnd < ssthresh，使用慢启动算法
- 否则，使用 **拥塞避免算法**



##### 拥塞避免算法

cwnd >= ssthresh 时，就会使用 拥塞避免算法

一般来说 ssthresh 大小为 `65535` 字节

拥塞比慢算法的规则：**每收到一个 ACK，cwnd  + 1**

这就由慢启动的 指数增长，转换为了 拥塞避免算法的 线性增长



##### 拥塞发生

一直增长，网络肯定有阻塞的时候，就会触发重传机制

- 超时重传

  此时会使用 **拥塞发生算法**：

  - ssthresh 会变为 cwnd / 2
  - cwnd 重置为 1

  **直接从拥塞，回到 慢启动阶段，反应很明显，会造成网络卡顿**

- 快速重传

  此时的 拥塞发生算法 不一样：

  - cwnd = cwnd / 2
  - ssthresh = cwnd
  - 进入快速恢复算法



##### 快速回复

快速重传和快速恢复算法一般同时使用

进入快速恢复算法之前，cwnd 和 ssthresh 已经更新了：

- cwnd = cwnd / 2
- ssthresh = cwnd

然后进入快速恢复算法：

- cwnd = ssthresh + 3（3 的意思是 确认有 3 个数据包收到了）
- 重传丢失的数据包
- 再收到重复的 ACK，cwnd + 1
- 如果收到新数据的 ACK，把 cwnd 设置为第一步中的 `ssthresh` 的值，原因是该 ACK 确认了新的数据，说明 丢失的数据都收到了，恢复过程结束，可以回到恢复前的状态了，再次进入拥塞避免状态



### TCP 三次握手异常情况处理

#### TCP 第一次握手 SYN 丢包

当客户端发起的 TCP 第一次握手 `SYN` 包，在 RTO（超时时间）内没有收到服务端的 `ACK`，就会触发超时重传 `SYN` 包，每次超时重传的 RTO 翻倍增加，直到 SYN 包的重传次数达到 `tcp_syn_retries`，或者得到 `ACK`，客户端就不再重传了



#### TCP 第二次握手 SYN + ACK 丢包

当 TCP 第二次握手的 `SYN + ACK` 包丢失后，客户端因为一直没接收到 `ACK` 所以触发超时重传；而 服务端因为没有接收到 `ACK`，也会超时重传

`SYN + ACK` 超时重传的次数由 `tcp_synack_retries` 决定，每次的 RTO 也是指数增长



#### TCP 第三次握手 ACK 丢包

流程如下：

- 客户端发送 `SYN` 报文给 服务端，服务端收到后，回应 `SYN + ACK` 报文给客户端，**此时服务端的 TCP 连接处于 `SYN_RECV` 状态**
- 客户端收到 `SYN + ACK` 报文后，给服务端回应 `ACK` 报文，**`此时客户端处于ESTABLISHED` 已连接状态**
- 由于 服务端迟迟收不到 客户端的 ACK 报文，所以 服务端一直处于 `SYN_RECV` 状态
- 然后服务端触发超时重传 `SYN + ACK` 包，超过 `tcp_synack_retries` 的值就不再重传了，此时，**服务端的 TCP 连接主动终止，连接断开；客户端依旧是 `ESTABLISHED` 已连接状态**
- 如果现在 客户端向服务端发送请求，由于 服务端已经断开了，所以不可能收到 `ACK` 报文，那么 客户端就会触发超时重传，每一次重传，RTO 都指数增长，持续一段时间后，报错退出连接

**TCP 建立连接后，数据包重传的最大超时重传次数由 `tcp_retries2` 指定，默认值 15 次**

> 保活机制：定义一个时间段，如果持续没有任何连接相关的活动，TCP 保活机制就会开始作用，每隔一个时间间隔，发送一个 **探测报文**，该报文数据非常少，如果连续几个 探测报文 都没有得到回应，就认为当前 TCP 连接已经死亡

相关参数：

- `tcp_keepalive_time`：保活时间，这个时间内没有任何连接活动，启动保活机制
- `tcp_keepalive_intvl`：每次探测报文的时间间隔
- `tcp_keepalive_probes`：最大探测报文检测次数，超过这个次数，就中断连接

如果此时 客户端不发送 请求，由于 TCP 的 **保活机制**，也会在一段时间后断开连接



#### TCP 快速建立连接

客户端在向服务端发起 HTTP GET 请求的时候

- 建立 TCP 连接，1.5 RTT
- 发送 HTTP GET 请求，接收响应，1 RTT

共 2.5 RTT

第三次握手可以携带数据，如果在 第三次握手 发起 HTTP GET 请求，就需要 2 RTT

- `SYN`
- `SYN + ACK`
- `ACK + HTTP GET`
- `data`

**Fast Open 可以减少 TCP 连接建立的时延**

- 第一次建立 TCP 连接的时候，服务端在第二次握手产生一个 `Cookie`（已加密，有有效时间），并随着 `SYN + ACK` 报文一并发送给客户端，客户端缓存下来，**首次建立 TCP 连接至少需要 2 RTT**
- 再次请求建立 TCP 连接的时候，**客户端可以在 SYN 包中带上 `Cookie` 和HTTP GET 的数据，一并发送给服务端，这样服务端就可以跳过三次握手，直接返回 `SYN + ACK + Data` 包**



### TCP 半连接队列 和 全连接队列

- 半连接队列：也称 SYN 队列
- 全连接队列：也称 accept

服务端收到 `SYN` 报文后，**内核会把该连接存储到半连接队列**，并向客户端响应 `SYN + ACK`，接着客户端返回 `ACK`，服务端收到后，**内核会把连接从半连接队列移除，然后创建新的 全连接，添加到全连接队列，等待进程调用 `accept` 函数把连接取出来**

**两个队列都有最大长度限制，超过会直接丢弃，或返回 RST 包**



#### 全连接队列溢出

当服务端并发处理⼤量请求时，如果 TCP 全连接队列过⼩，就容易溢出。发⽣ TCP 全连接队溢出的时候，后续的请求就会被丢弃，这样就会出现服务端请求数量上不去的现象

**丢弃连接是 默认行为，还可以选择向 客户端发送 RST 报文**

**注意：如果直接丢弃连接，那么客户端此时是 ESTABLISHED 已连接状态，如果服务器短暂繁忙，客户端重传就有可能在全连接队列有空位时，成功与服务端建立连接；否则，发送 RST 报文就会断开连接**



#### 半连接队列溢出

半连接队列溢出最常见的情况就是，SYN 攻击，DDos 攻击

##### 如何防御 SYN 攻击

- 增大半连接队列

  同时增大 半、全连接队列

- 开启 `tcp_syncookies` 功能

  **开启 `syncookies` 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接**

  服务器根据当前状态计算一个值，放入 `SYN + ACK` 报文中，客户端返回 ACK 报文时，取出该值验证，合法就认为连接建立成功

  - 0，关闭
  - 1，SYN 半连接队列溢出时启用
  - 2，无条件启用

- 减少 `SYN + ACK` 重传次数

  半连接队列溢出是因为有太多 处于 `SYN_REVC` 状态的 TCP 连接，他们都在重传 `SYN + ACK`，重传超过次数，才会断开连接

  那么，减少重传次数，缩短这个时间，就可以缓解 `SYN` 攻击



### TCP 三次握手性能优化

| 策略                          | TCP 内核参数                            |
| ----------------------------- | --------------------------------------- |
| 调整 SYN 报文的重传次数       | tcp_syn_retries                         |
| 调整 SYN + ACK 报文的重传次数 | tcp_synack_retries                      |
| 调整 半连接队列 长度          | tcp_max_syn_backlog、somaxconn、backlog |
| 调整 全连接队列 长度          | min(backlog, somaxconn)                 |
| 绕过三次握手                  | tcp_fastopen                            |



## IP

------

### 基本认识

****

IP 在 TCP/IP 参考模型中处于网络层

网络层的主要作用是：实现主机与主机之间的通信（点对点通信）

#### 网络层与数据链路层有什么关系？

网络层（IP）的作用是主机之间通信用，而数据链路层（MAC）的作用是实现直连的两个设备之间通信，IP 负责在没有直连的两个网络间通信

**源IP地址和⽬标IP地址在传输过程中是不会变化的，只有源 MAC 地址和⽬标 MAC ⼀直在变化**



### IP 地址的基础知识

****

IPv4 由 32 位二进制数来表示，通常用 **点分十进制** 来表示

点分十进制就是 32 位 IP 地址，每八位一组，每组用十进制表示，组与组之间用 点 分隔开

IPv4 最多允许 2^32 - 1 台计算机接入到网络

实际上，IP 地址也不是根据主机台数来配置的，所以实际能接入的计算机个数比理论值少很多



#### IP 地址的分类

IP 地址共分为 A/B/C/D/E 五类



##### 什么是 A/B/C 类地址？

A/B/C 类地址主要分为两个部分，分别是 **网络号 和 主机号**

- A 类地址 最高位 是 0，然后 是 7 位的网络号，24 位的主机号
- B 类地址 前两位是 10，然后是 14 位的网络号，16 位的主机号
- C 类地址 前三位是 110，然后是 21 位的网络号，8 位的主机号

最大主机个数要看主机号的位数，比如 C 类 主机号 8 位

那么 C 类的最大主机个数就是， 2^8 - 2 = 254

为什么 - 2 ？

- 主机号全是 1：用于指定某个网络下的所有主机，用于广播
- 主机号全是 0：指定某个网络



##### 什么是 D/E 类地址？

D 类常用于 多播（将包发送给特定组内的所有主机），E 类是预留的分类，暂未使用



##### IP 分类的优点

简单明了、选路（基于网络地址）简单



##### IP 分类的缺点

- 同一网络下没有地址层次

  ⽐如⼀个公司⾥⽤了 B 类地址，但是可能需要根据⽣产环境、测试环境、开发环境来划 分地址层次，⽽这种 IP 分类是没有地址层次划分的功能，所以这就**缺少地址的灵活性**

- 不能很好的与现实网络匹配

  例如

  C 类地址太少，不够用

  A 类地址太多，用不了



#### 无分类地址 CIDR

为了解决 分类地址的诸多缺点，提出了 无分类地址 `CIDR`

32 位被分为两部分：网络号 和 主机号



##### 如何划分网络号和主机号？

表示形式 `a.b.c.d/x`，其中 `/x` 表示前 `x` 位属于网络号，`x` 的范围是 `0~32`，这就使 IP 地址更加具有灵活性

比如 `10.100.122.2/24`，这种表现形式就是 CIDR，前 24 位是网络号， 后 8 位是主机号

还有一种形式就是 **子网掩码**，掩码的意思就是掩盖掉主机号，剩余的是网络号

**将子网掩码与 IP 地址 按位与计算，得到的就是网络号**

比如 IP 地址是 `10.100.122.2`；子网掩码是 `255.255.255.0`

二者按位与 得到：`10.100.122.0`，那么网络号就是 `10.100.122.0`



##### 为什么要分离网络号和主机号？

两台计算机通讯的时候，**首先判断二者是否处在同一个广播域内，即网络号是否相同；在同一个广播域内可以直接把数据包发送到目标主机**

路由器寻址⼯作中，也就是通过这样的方式来找到对应的⽹络号的，进而把数据包转发给对应的网络内



##### 怎么进行子网划分？

**子网划分把主机号分为了两个部分：子网网络地址 和 子网主机地址**

未做子网划分的 IP 地址：网络地址 + 主机地址

做子网划分的 IP 地址：网络地址 + （子网网络地址 + 子网主机地址）

假设对 C类地址进行子网划分

⽹络地址 **192.168.1.0**，使⽤⼦⽹掩码 **255.255.255.192** 对其进⾏⼦⽹划分

C 类地址，24 位网络地址，8 位主机地址

从子网掩码来看，**8 位的主机地址，前两位 被拿来做 子网网络地址；后六位是 子网主机地址**

划分后：

| 子网号 | 网络地址      | 子网主机地址范围            | 广播地址      |
| ------ | ------------- | --------------------------- | ------------- |
| 00     | 192.168.1.0   | 192.168.1.1~192.168.1.62    | 192.168.1.63  |
| 01     | 192.168.1.64  | 192.168.1.65~192.168.1.126  | 192.168.1.127 |
| 10     | 192.168.1.128 | 192.168.1.129~192.168.1.190 | 192.168.1.191 |
| 11     | 192.168.1.192 | 192.168.1.193~192.168.1.254 | 192.168.1.255 |



#### 公有 IP 和 私有 IP

在 A/B/C 分类地址，实际上分 公有 IP 和 私有 IP

- 公有 IP 不能重复，统一分配（由 `ICANN` 统一分配）
- 私有 IP 可以重复，由组织内自己分配



#### IP 地址与路由控制

IP 地址的 **网络地址** 是用于进行路由控制的

> 路由控制表：记录着网络地址与下一步应该发送至路由器的地址。在主机和路由器上都有各自的路由器控制表

在发送 IP 包的时候，首先确定 IP 包首部中的目标地址，再从路由控制表中找到与该地址具有 **相同网络地址** 的记录，根据该记录把 IP 包转发给相应的下一个路由器。如果路由控制表中存在多条相同网络地址的记录，就 **选择相同位数最多的网络地址，也就是最长匹配**，**如果没有匹配到，就转发到默认路由**



##### 环回地址是不会流向网络的

**环回地址** 是在同⼀台计算机上的程序之间进行网络通信时所使⽤的⼀个默认地址

`127.0.0.1` 在计算机中作为环回地址，也被称为 `localhost`，使用这个 IP 或主机名时，数据包不会流向网络



#### IP 分片与重组

每种数据链路的 最大传输单元 MTU 是不同的，以太网 一般为 1500 字节

不同是因为 **不同类型的数据链路使用目的不同**

IP 数据包大小超过 MTU 的时候，IP 数据包就会分片；分片后的 IP 数据只能由目标主机进行，路由器不会重组

IP 层的 分片传输中，某个分片丢失，整个 IP 数据作废，这是很不友好的；所以 TCP 引入了 `MSS`，即在 TCP 层对数据进行分片，即便丢失只需要 TCP 重传丢失的数据包即可；UDP 的话，尽量不要使得单次的报文长度超过 MTU



#### IPv6 基本认识

IPv4 理论上大约可以提供 42 亿个地址，但是 2011 年 IPv4 的地址就已经分配完了

IPv6 的地址是 128 位的，这个地址数量不用再担心会被分配完了

**IPv6 除了 地址数量更多，还有更好的安全性和扩展性，简单来讲， IPv6 体验更好；只不过 IPv4 和 IPv6 不能兼容，除了使用设备需要升级，运营商的设备也要升级**



##### IPv6 的亮点

- 地址数量更多
- IPv6 可自动配置，即使没有 DHCP 服务器也可以自动分配 IP 地址，**便捷**
- IPv6 包首部长度采用 **固定值 `40` 字节**，去掉了 首部校验和，简化了 首部结构，减轻了 路由器负荷，**提高了传输性能**
- IPv6 有应对 伪造 IP 地址的网络安全功能以及防止线路窃听的功能，提高了 **安全性**



##### IPv6 地址的标识方法

- IPv4 地址长度共 32 位，每 8 位作为一组，点分十进制表示
- IPv6 地址长度共 128 位，每 16 位作为一组，每组用冒号隔开
  - 每组使用 十六进制 表示
  - 如果出现连续几组全都是 0 的话，还可以把这些 0 省略，并用 两个冒号 隔开，**一个地址中只允许出现一次两个连续冒号**



##### IPv6 地址结构

IPv6 类似 IPv4，也是通过 IP 地址的前⼏位标识 IP 地址的种类

- 单播地址：用于 一对一通信
- 组播地址：用于 一对多通信
- 任播地址：用于通信最近的节点，最近的节点由路由协议决定
- 没有广播地址



##### IPv6 单播地址类型

主要有三类单播地址，各自的有效范围不同

- 链路本地单播地址：在同一链路单播通信，不经由路由器，IPv4 没有
- 唯一本地地址：在内网里单播通信，相当于 私有 IP
- 全局单播地址：在互联网通信，相当于 公有 IP



#### IPv4 和 IPv6 首部差异

IPv6 相⽐ IPv4 的⾸部改进：

- 取消了 首部校验和 字段：因为 数据链路层 和 传输层都会校验
- 取消了 分片/重新组装 相关字段：IPv6 不允许在中间路由器进行 分片/重组 操作，只能在 源与目标主机进行，提高了转发速度
- 取消了 选项字段：该字段内容可能出现在 下一个首部（Next Header） 字段，删除该字段使得 IPv6 的首部的长度固定为了 40 字节



### IP 协议相关技术

****

- DNS 域名解析
- ARP 与 RAPR 协议
- DHCP 动态获取 IP 地址
- NAT 网络地址转换
- ICMP 互联网控制报文协议
- IGMP 因特网组管理协议



#### DNS 域名解析

**DNS 域名解析** 可以把 域名网址 自动转化成 IP 地址

##### 域名的层级关系

DNS 中的域名都是用 **句点** 分割的，比如 `WWW.server.com`，句点代表了不同层次之间的界限

域名中，**越靠右，层级越高**

> 根域名：最高一级的域名节点，比如 `www.example.com` 被写成`www.example.com.`，即最后还会多出一个点。这个点就是根域名；根域名才知道 顶级域名 是由谁托管的

域名的层级管理类似一个树状结构：

- 根 DNS 服务器
- 顶级域 DNS 服务器（比如 com）
- 权威 DNS 服务器（比如 server.com）

任何一台 DNS 服务器都可以找到 根域 DNS 服务器，然后再找到下层的某台目标 DNS 服务器



##### 域名解析的工作流程

- **浏览器缓存**中有没有
- **操作系统的缓存**中有没有
- **本机域名解析文件**有没有
- DNS 服务器查询
  - 客户端发出 DNS 请求，询问 `www.server.com` 的 IP 地址是啥，发送给**本地 DNS 服务器**（客户端的 TCP/IP 设置中填写的 DNS 服务器地址）
  - 本地 DNS 服务器收到请求，如果缓存的表格中有 该域名，直接返回 IP 地址；如果没有，本地 DNS 服务器会访问 根域名服务器，去找对应的 IP
  - **根域名服务器** 收到 本地 DNS 服务器的请求后，发现该域名的顶级域名是 `com`，就返回 托管 `com` 顶级域名 的 服务器地址给 本地 DNS 服务器
  - 本地 DNS 服务器收到 **顶级域名服务器** 的地址后，向其发起请求，询问 IP 地址
  - 顶级域名服务器收到请求后，返回 `server.com` 的 **权威 DNS 服务器**，**它是域名解析结果的出处**
  - 权威 DNS 服务器查询到对应的 IP 地址后返回给 本地 DNS 服务器
  - 本地 DNS 服务器把 IP 地址 返回给客户端



#### ARP 地址解析协议

确定 源 IP 地址 和 目标 IP 地址后，就会通过主机路由表确定 IP 数据包的下一跳。网络层的下一层是 数据链路层，所以得知道下一跳的 MAC 地址

主机的路由表中可以找到下一跳的 IP 地址，所以可以通过 ARP 协议，求得下一跳的 MAC 地址

##### ARP 如何求得 MAC 地址的

简单来讲，ARP 协议通过 ARP 请求 和 ARP 响应两种类型的包确定 MAC 地址的

- 主机会 **广播发送 ARP 请求**，包中包含了 目标 IP 地址
- 同个 链路 中所有设备都会收到 ARP 请求，然后把包中的 IP 地址与自己的 IP 地址作比较，一致就把自己的 MAC 地址 放入 ARP 响应包，返回给主机

操作系统通常会把第一次通过 ARP 获取到的 MAC 地址缓存下来（有期限）

##### RARP 协议是什么

ARP 协议是 已知 IP 地址，求 MAC 地址

RAPR 协议是 已知 MAC 地址，求 IP 地址

通常需要架设一台 RARP 服务器，用以注册设备的 MAC 地址及其 IP 地址，然后把设备接入网络：

- 该设备会向 RARP 服务器发送 RARP 请求包，包中有 该设备的 MAC 地址
- RARP 服务器接收到请求之后，就会返回给该设备 对应的 IP 地址



#### DHCP 动态主机配置协议

DHCP 协议用于 动态获取 IP 地址，大大省去了配 IP 信息繁琐的过程（在主机接入网络的时候）

- 主机发起 **DHCP 发现报文（DHCP DISCOVER）** 的 IP 数据包，因为 主机现在没有 IP 地址，也不知道 DHCP 服务器的地址，所以无法建立 TCP 连接，只能发送 **UDP 广播** 通信，广播地址（目标 IP 地址）是 `255.255.255.255`，并且 源 IP 地址设置的是 `0.0.0.0`

- DHCP 服务器接收到 DHCP 发现报文后，用 **DHCP 提供报文（DHCP OFFER）** 向主机做出响应。该报文仍然使用广播地址（目标 IP 地址） `255.255.255.255`，源 IP 地址设置为 DHCP 服务器地址。

  报文中携带：

  - 可租约的 IP 地址
  - 子网掩码
  - 默认网关
  - DNS 服务器
  - IP 地址租用期

- 主机收到 一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向其发送 **DHCP 请求报文（DHCP REQUEST）** 进行相应，回显配置的参数

- 最后，服务器用 **DHCP ACK 报文** 对 DHCP 请求报文进行相应，应答所要求的参数

主机收到 DHCP ACK 报文后，交互就完成了，主机就可以在租期内使用分配的 IP 地址

如果租约快到期了，主机会向服务器发送 DHCP 请求报文：

- 服务器同意续租，响应 DHCP ACK 报文，延长租期
- 不同意续租，响应 DHCP NACK 报文，主机停止使用 当前 IP 地址

**DHCP 交互中，全程使用 UDP 广播通信**

因为有可能 DHCP 服务器和主机不在一个局域网中，**路由器不会转发广播包**，所以就有了 **DHCP 中继代理**，这使得 **不同⽹段的 IP 地址分配也可以由⼀个 DHCP 服务器统⼀进⾏管理**

- 主机会向 DHCP 中继代理 发送 DHCP 请求包，DHCP 中继代理收到广播包后，以 单播 的形式发送给 DHCP 服务器
- 服务器收到该包后 向 DHCP 中继代理发送应答，然后由 DHCP 中继代理广播给 主机

主机 与 中继代理 广播

中继代理 与 DHCP 服务器 单播



#### NAT 网络地址转换

因为 IPv4 地址依然有被耗尽的风险，所以提出了一种 网络地址转换 的方法，NAT

NAT 会把私有 IP 转化为 公有 IP，但是 普通的 NAT 转换没意义



由于绝大多数网络应用都是使用传输层协议 TCP 或 UDP 来传输数据的

所以，可以把 IP 地址 + 端口号 一起进行转换。这样用一个 全球 IP 就可以了，这种技术就叫做 **网络地址与端口转换 NAPT**

场景：

- 现在有一个 私有 IP 地址网络中，两台主机同时发送请求，且端口号相同
- 经过 NAT 转换之后，私有 IP 被转换成了 公有 IP，也就是说这两个请求的 源IP 地址 相同了，如果端口再相同就没法区分了，所以会以不同的端口号作为区分
- NATP 路由器中会自动生成一个 转换表，以保证正确转换 地址，**TCP 连接 首次握手时，SYN 包一经发出就会生成一条转换记录；随着收到关闭连接时发出的 FIN 包的确认应答从表中删除**



##### NAT 的缺点

NAT/NATP 都依赖自己的转换表：

- 外部无法主动与 NAT 内部服务器建立连接，因为 NATP 转换表没有转换记录
- 转换表的生成与转换操作都会产生性能开销
- 通信过程中，如果 NAT 路由器重启了，所有 TCP 连接都会被重置



##### 如何解决 NAT 潜在的问题

- 改用 IPv6

- NAT 穿透技术

  主机会主动从 NAT 设备获取 公有 IP 地址，自己建立映射条目，然后使用这个映射对外通信，就不需要在 NAT 进行转换了



#### ICMP 互联网控制报文协议

> ICMP 用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息

##### ICMP 的功能有什么

主要功能包括：

- 确认 IP 包是否成功送达目标地址
- 报告发送过程中 IP 包被废弃的原因
- 改善网络设置等

如果 IP 通信中，**某个 IP 包因为某种原因未能到达目标地址**，那么这个具体的原因会**由 ICMP 负责通知**

ICMP 的 通知消息会使用 IP 进行发送



##### ICMP 的类型

- 用于诊断的查询消息，**查询报文类型**
- 通知出错原因的错误消息，**差错报文类型**

|      | 内容                                  | 种类         |
| ---- | ------------------------------------- | ------------ |
| 0    | 回送应答（Echo Reply）                | 查询报文类型 |
| 8    | 回送请求 （Echo Request）             | 查询报文类型 |
| 3    | 目标不可达（Destination Unreachable） | 差错报文类型 |
| 4    | 原点抑制（Source Quench ）            | 差错报文类型 |
| 5    | 重定向或改变路由（Redirect）          | 差错报文类型 |
| 11   | 超时（Time Exceeded）                 | 差错报文类型 |



#### IGMP 因特网组管理协议

D 类地址是 组播地址，既然是组播，就需要一种手段来管理分组，这就是 IGMP

IGMP 是因特⽹组管理协议，⼯作在主机（组播成员）和最后⼀跳路由之间

- IGMP 报文想路由器申请加入、退出组播组
- IGMP 报文用 IP 封装，IP 头部 的协议号为 2，TTL 字段值通常为 1



##### IGMP 工作机制

IGMP 分为了三个版本分别是，IGMPv1、IGMPv2、IGMPv3

下面的场景基于 v2

常规查询与响应工作机制

- 路由器会周期性发送 目的地址为 `224.0.0.1`（表示同一网段内所有主机和路由器）的 **IGMP 常规查询报文**
- 组成员收到查询后，会启动 报告延时计时器，计时器的时间是随机的，通常是 0~10 秒，计时器 **超时** 主机就会发送 **IGMP 成员关系报文**（源 IP 地址是自己主机 IP 地址，目标 IP 地址是组播地址）。定时器超时之前，收到同组内其他主机发送的 成员关系报文，自己就不发送了，可以减少网络中冗余的 IGMP 报文数量
- 路由器收到主机的成员关系报⽂后，就会在 IGMP 路由表中加⼊该组播组，后续⽹络中⼀旦该组播地址的数据 到达路由器，它会把数据包转发出去



离开组播组工作机制

- 网段中仍有该组播组
  - 组播组成员之一要离开组，发送 **IGMPv2 离组报文**，报文的目的地址是 `224.0.0.2`（网段中所有路由器）
  - 路由器收到后，以 1 秒 为间隔连续发送 IGMP 特定组查询报文（共 2 个），以确认该网段中是否还有该组的其他成员
  - 如果有，则该成员会响应这个特定组查询，路由器就知道这个组还有成员，会继续向该网段转发该组的组播数据包
- 网段中没有该组播组
  - 前两步一样
  - 没有成员响应特定组查询，路由器认为该组播组没有了，就不再向该网段转发该组的组播数据包了





## 网络综合

------

### 键入网址到网页显示，期间发生了什么？

****

1. URL 解析-生成 HTTP 数据包

   第一步工作就是对 `URL` 进行解析，从而生成请求信息

   URL 由 协议、域名、路径名组成

   > 如果没有路径名，那就代表访问根目录下设置的 **默认文件**

   URL 解析之后，浏览器确定了 域名 和 文件名，接下来就是生成 HTTP 数据包了

2. DNS 域名解析-使用 UTP 协议

> 协议栈
>
> 获得 目的 IP 后，就可以把 HTTP 的传输工作交给 操作系统 中的 协议栈 了
>
> - 上半部分：两部分，一部分负责 TCP 协议；另一部分负责 UDP 协议
>
> - 下半部分：用 IP 协议 控制网络包收发操作，**在互联网上传数据时，数据会被切分成一块块的网络包，而将网络包发送给对方的操作就是由 IP 负责的**
>
>   IP 中还包括 `ICMP` 协议 和 `ARP` 协议
>
>   - `ICMP` 用于告知网络包传送过程中产生的错误以及各种控制信息
>   - `ARP` 用于根据 IP 地址查询 MAC 地址



3. TCP 建立连接，生成 TCP 报文（TCP 头部 + HTTP 报文）

> IP 层封装网络包，TCP 模块在连接、收发、断开操作时，都需要委托 IP 模块把数据封装成网络包（IP 头部 + TCP 报文），IP 头部会有 源 IP 地址、目标 IP 地址，借此可以远程定位

> IP 头部生成之后，还要在 IP 头部前面加上 MAC 头部
>
> - 发送方 MAC 地址 就是本地主机，这个很好获取
> - 接受方 MAC 地址 就需要 ARP 协议寻找了
>   - 如果 目标主机是本地局域网，也就是 IP 地址中的网络标识相同，就直接 ARP 协议，广播获取 目标 MAC 地址
>   - 否则，目标 MAC 地址就填 路由器，让路由器根据路由表去转发，直到 目标 IP 是 该路由器局域网内的主机
>
> 网络包的报文就全部生成了（MAC 头部 + IP 头部 + TCP 报文）

> 网卡会把 网络包 复制到自己的缓冲区，在开头加上 **报头和起始帧分界符（表示包起始位置）**，在末尾加上 **FCS（帧校验序列）（检查包是否损坏）**，然后转化为电信号，发送出去

> 交换机
>
> 1. 通过 FCS 校验包有没有问题，没有问题就放到自己的缓冲区
> 2. **交换机端口没有 MAC 地址**
> 3. 查询 目标 MAC 地址在不在 MAC 地址表 中；有的话直接发送到相应端口；没有就给源端口之外所有端口都发。只有目标 MAC 会回应；如果 目标 MAC 本来就是 广播地址，那就除了源端口都发

> 路由器
>
> 路由器基于 IP 设计，**各个端口都有 MAC 地址 和 IP 地址**
>
> 1. 通过 FCS 校验包有没有问题
> 2. 查询 MAC 头部的 目标 MAC 地址，如果是发给自己的，就放到自己的缓冲区；否则丢弃
> 3. 到达目标路由器之后，查询路由表确定输出端口
> 4. 先匹配 目标 IP 地址的 网络标识，匹配到的就是输出端口。然后看它的网关，**如果网关是一个 IP 地址，说明需要继续转发，那这个 IP 地址的 MAC 地址就是下一跳的 MAC 地址；否则 IP 头部中的 接收方地址就是要转发的 目标 IP 地址，它的 MAC 地址就是 下一跳的 MAC 地址**，然后通过交换机，到达下一个路由器，重复这个过程
>
> **路由器也有 ARP 缓存**

4. SSL/TLS 建立连接
5. 请求在进入 服务端之前可能会经过负载均衡的服务器，作用是把请求合理的分发到多台服务器上；然后返回数据，比如是一个 HTML 文件
6. 客户端接收到之后，浏览器先判断状态码：
   1. 200 成功，继续解析
   2. 4xx,5xx，报错
   3. 304 命中协商缓存，直接从缓存里取出上一次的数据
   4. 3xx 重定向，**重定向有计数器，超过次数也会报错**
7. 如果是压缩过的先解压，然后根据编码格式解码文件
8. 开始渲染，先根据 HTML 构建 DOM 树，有 CSS 构建 CSSOM 树
9. 如果遇到 script 标签，会判断 同步还是异步
   1. async 属性，并行下载并执行 JS
   2. defer 属性，先下载文件，HTML 解析完成后，顺序执行
   3. 都没有，**阻塞渲染流程，先下载执行 JS**
10. 初始 HTML 完全加载解析后会触发 `DOMContentLoaded` 事件，不需要等待 CSS，图片，子框架的完全加载
11. DOM 树 和 CSSOM 树 构建完成后，会开始生成 Render 树，这个过程中，也会开始 GPU 渲染









### 前端鉴权

------

基于 HTTP **无状态** 的特性，在某些场景下，我们需要维护状态，最典型的就是用户登录，用以访问某些信息

无状态，但是需要维护状态，又不能频繁发送 HTTP 请求，那就只能把状态保存在前端：

- 挂载到全局变量上，刷新页面就没了
- cookie、localStorage 等里，方便访问，而且不会随着刷新过期

通常是使用 cookie，因为我们保存状态的目的也是再接下来的请求中，让服务端能够识别我们，那就需要在 HTTP 请求中带上 状态信息，cookie 不需要我们手动操作，设置好之后，会在对应的 Domain/Path 的请求发送时自动带上

#### 应用方案：服务端 session

- 客户端发送账号密码，服务端验证
- 验证成功就把用户状态存为 session，生成一个 sessionId
- 把 sessionId 放到 Set-Cookie 响应头中，返回给客户端
- 之后客户端再请求业务接口，sessionId 就会自动添加到 Cookie 中
- 服务端查询 sessionId，验证 session，通过就正常处理业务，返回结果

session的过期与销毁：可以在 session 中加入过期时间，到期就删除库中的数据



#### 应用方案：Token

session 的方案有一个痛点，就是需要服务端维护 session 信息，但这其实是没有必要的，session 中的信息其实可以直接反应在 cookie 中，然后在业务代码中直接验证

- 客户端登录，服务端验证用户信息
- 把必要的信息，比如用户身份信息，状态信息，token 过期时间等，编码成 token，然后放到 Set-Cookie 响应头中，返回给客户端
- 之后客户端再请求业务接口，token 就会自动添加到 Cookie 中
- 验证 token，通过就正常处理业务

#### **防篡改**

如果 token 只是由信息直接简单编码，那就很容易被人得知信息，进而伪造 token进行非法访问

如果 token 涉及到敏感权限，就要想办法避免这样的事

解决方案：给 token 加签名，然后通过加密算法复杂编码



#### JWT

token 的防篡改会额外增加 cookie 的体积，加大服务端的压力，JWT 就是解决这个问题的

> JSON Web Token (JWT) 是一个开放标准，定义了一种传递 JSON 信息的方式。这些信息通过数字签名确保可信



#### refresh token

业务接口用来鉴权的 token 被称为 access token，权限敏感的话，我们就希望有效期尽可能短，防止被盗用，但是太短的话，又会造成频繁验证身份

解决办法就是，用另一个 token 生成 access token，我们称之为 refresh token

- 客户端登录，服务端验证用户信息
- 验证通过，生成 refresh token，生成 access token，返回给客户端
- 之后客户端再请求业务接口，access token 就会自动添加到 Cookie 中
- 如果 access token 过期，就请求生成新的 access token
- 获得新的 access token 之后再请求业务接口

**refresh token 的作用就是，access token 频繁的过期生成，不再需要与数据库进行交互，这就节省了大量时间**



#### session 与 token

在浏览器端，可以用 cookie，如果没有 cookie 的话，就用别的前端存储方案，而且，**cookie 容易引发CSRF 攻击**

session 和 token 在浏览器端的状态存储，一般都选择 cookie

服务端来说：

- session 存数据，请求只需要携带 sessionId，可以减少请求头部信息，但是需要与 库交互，验证 session
- token 不存数据，所有的数据就在 请求 中，不需要服务端整套的解决方案和分布式处理，也减少了与库交互带来的性能开销

#### 单点登录

同域下的客户端/服务端认证系统中，客户端携带凭证，以维护状态信息

但是如果业务系统过大，分散到了不同的域名下，就需要 **一次登录，到处使用** 的能力，这就是 **单点登录**

**注意，只有 SSO 系统有登陆功能，其他系统负责各自的业务功能**

- 如果整套业务系统都在一个顶级域名下，那只需要把 cookie 的 domain 字段设置为公用的顶级域名，就实现了 单点登录

- 如果并不都在一个主域名下，就需要实现 单点登录系统（SSO）了

  非浏览器（不需要考虑跨域限制）

  - 客户端访问 A 系统，未登录状态，重定向到 SSO 系统
  - SSO 系统也是未登录状态，用户填写信息进行登录，如果验证成功，就返回给客户端一个 登录凭证（ticket）
  - 因为没有跨域限制，所以请求其他系统的时候，该登录凭证是通用的，**但是，验证登陆凭证是在 SSO 系统**

  浏览器（有跨域限制）

  - 客户端访问 A 系统，未登录状态，重定向到 SSO 系统
  - SSO 系统也是未登录状态，用户填写信息进行登录，如果验证成功，就可以完成 **客户端与SSO 系统的鉴权** 了，即客户端存储了 SSO 系统这个 域下的 cookie
  - SSO 系统完成鉴权后，还会生成一个 ST（Service Ticket），然后跳转回 A 系统，并将 ST 作为参数传递给 A 系统
  - A 系统拿到 ST 后，向 SSO 系统发送请求，验证 ST
  - 验证通过， **A 系统将登录状态写入到客户端 A 系统域下的 cookie**，这样，A 系统和客户端的鉴权也完成了
  - 不同系统的 cookie 肯定不同，因为有跨域限制

  不管是不是在浏览器，每次向业务系统发送请求时，**后台向 SSO 发送请求验证身份都是必要的**，如果不进行验证的话，是可以进行伪造信息的



### DNS 预解析

所有的网络请求都需要进行 DNS 解析，得到 目的 IP 地址

这个过程有的时候是很易感知的

所以就有了 DNS 预解析，即让浏览器主动执行域名解析的工作，而不是请求发送的时候再解析

如何开启

- HTML 的 head 标签中，添加一个 meta 标签

```html
<meta http-equiv="x-dns-prefetch-control" content="on">
```

然后需要预解析哪些域名，就把它在 head 标签中写一个 link 标签，按照以下形式

```html
<link rel="dns-prefetch" href="https://lf-cdn-tos.bytescm.com/"
```

- 页面中的 a 标签的 href 都会自动预解析，不需要手动预解析了
- dns-prefetch 适用于网页引用了大量其他域名的资源，比如淘宝这种



### CDN

CDN 通过把 内容 发布到遍布全球的 海量加速节点，使其用户可就近获取所需内容，避免因 网络拥堵、跨运营商、跨地域、跨境等因素带来的网络不稳定、访问延迟高等问题，有效提高了下载速度、降低响应时间、提升用户体验

优点：

- CDN 节点解决了 **网络拥堵、跨运营商、跨地域、跨境** 等问题，访问延时大大降低
- 大部分请求在 CDN 边缘节点完成，CDN 起到了 **分流作用，减轻了源站的负载**

缺点：内容更新时，如果 CDN 节点上数据没有及时更新，即使用户清空浏览器缓存，也会因为 CDN 边缘节点没有同步最新数据而导致用户访问异常



#### CDN 加速原理

用户请求 CDN 服务的资源时，本地 DNS 服务器通过 CNAME 把 最终域名请求重定向到 CDN 服务，CDN 通过 调度系统的智能解析返回响应用户最快的 CDN 节点，然后进行交互



#### CDN 缓存策略

CDN 边缘节点的缓存 通过 HTTP 响应头中的 Cache-Control: max-age 字段来设置数据缓存时间

客户端向 CDN 节点请求数据时，CDN 节点会判断 本节点的缓存是否过期，没有过期就直接返回给客户端；否则，CDN 节点就会向源站发出 回源请求，从源站拉取最新数据，更新本地缓存，把最新数据返回给客户端

HTTP 请求流程：

1. DNS 域名解析
2. 权威 DNS 服务器解析发现域名 CNAME 到了 CDN 提供的 的 CNAME域名
3. 请求发向 CDN 服务，CDN 对 域名 智能解析，把响应速度最快的 CDN 节点 IP 地址 返回给 本地 DNS 服务器
4. 向这个 IP 地址发送请求
5. 边缘节点判断缓存是否有效
   1. 有效，把资源返回给用户
   2. 无缓存或缓存失效，CDN 边缘节点 从源站拉取最新资源，缓存至节点，返回给客户端



### HTTP 缓存

#### 强制缓存

在**缓存未过期**的时候，浏览器不会再发出请求，直接使用缓存数据

一般用 Expires，Cache-Control: max-age

- 状态码 200
- 性能好
- 缺点是，如果这期间 服务端资源更新了，页面上拿不到

Chrome 中：

- 更新频率高的资源，缓存在 内存中
- 更新频率低的，缓存在 磁盘中



#### 协商缓存

**强制缓存过期**了，或者**不走强制缓存策略的资源**，当第二次请求的时候，客户端就会与服务端协商，对比资源是否更新

1. 强制缓存过期，max-age 或 Expires 到期了
2. Cache-Control: no-cache

第一次的响应头中有：

- Etag：一个 hash 值，资源标识，但是在分布式环境中可能不同
- Last-Modified：根据上一次资源修改时间判断

请求头中带上：

- If-Not-Match：对应 Etag，就是那个 hash 值，与服务端资源作比较
- If-Modified-Since：对应 Last-Modified，与服务端作比较

- 未更新，返回 304，直接使用缓存资源
- 更新，返回 200，使用新请求的资源



#### 总结

preload 的资源一般是内存缓存，其他资源一般是硬盘缓存

1. 第一次请求，根据 响应头中的 `pragma/cache-control/expires`，判断是禁止缓存、强制缓存、协商缓存
   - cache-control: no-store，禁止缓存
   - cache-control: no-cache、pragma: no-cache，协商缓存
   - 其他的都是强制缓存
2. 第二次请求
   - 禁止缓存，直接请求资源
   - 强制缓存，没过期，直接用缓存资源
   - 强制缓存过期，协商缓存
     - 第一次响应头中有 `Etag`，请求头就带上 `If-None-Match`，值一样
     - 响应头有 `Last-Modified`，请求头就带上 `If-Modified-Since`，值一样
     - 根据这两个请求头，判断服务器资源是否更新
       - 未更新，返回 304
       - 更新，返回新资源，200



### 同源策略

> 同源：如果两个 URL 的 **协议、域名、端口号** 都相同，就是同源的
>
> HTTP **Origin** 头部存储的就是发起该请求的站点的 源

同源策略限制：

- Cookie、localstorage、indexedDB 等存储性内容
- DOM 节点
- AJAX 请求能够发送，但是返回被拦截了

三个标签允许跨域加载资源：

- img
- link
- script



#### JSONP

利用 `script` 标签没有跨域限制的漏洞，网页可以得到从其他来源动态产生的 JSON 数据。JSONP请求一定需要对方的服务器做支持才可以

优点：简单、兼容性好

缺点：仅支持 GET 方法（因为需要把回调函数作为参数传递给）

实现：

1. 写一个回调函数，这个函数的参数，需要其他域来提供
2. 我们把这个 函数名，当做参数，放到 跨域 API 的URL 中，就像 GET 请求的参数
3. 跨域 API 那边，把想要给我们的数据转成 JSON 字符串，和 回调函数名 拼接到一起，返回给我们
4. 接收到之后，相当于以 那边的 数据为参数，调用回调函数，这样就实现了 跨域传递信息

Jquery 也能实现 JSONP

```javascript
function createScript(url){
  const script = document.createElement('script');
  script.src = url
  script.type = 'text/javascript'
  return script
}

function jsonp(url, onsuccess, onerror){
  const hash = Math.random().toString().slice(2);
  onsuccess = typeof onsuccess === 'function' ? onsuccess : (data) => console.log(data);
  onerror = typeof onerror === 'function' ? onerror : (reason) => {throw new Error(reason)}
  window[`jsonp${hash}`] = function(data){
    onsuccess(data)
  }
  const script = createScript(`${url}?callback=jsonp${hash}`)
  script.onload = function(){
    script.onload = null
    script.parentNode.removeChild(script)
    delete window[`jsonp${hash}`]
  }
  script.onerror = function(reason){
    onerror(reason)
    script.parentNode.removeChild(script)
    delete window[`jsonp${hash}`]
  }
  document.body.appendChild(script)
}
```



#### CORS

CORS 需要 浏览器和后端同时支持，IE 8 和 9 需要通过 XDomainRequest

浏览器自动进行 CORS 通信，该方法主要在后端

**服务端需要设置 Access-Control-Allow-Origin**，该属性表示哪些域名可以访问资源，如果设置通配符，就表示所有网站都可以访问

使用 CORS 请求会出现两种情况

- 简单请求
  - 使用 GET/HEAD/POST HTTP 方法
  - 头部信息只有以下几种字段
    - Accept
    - Accept-Language
    - Conente-Language
    - Last-Event-ID
    - Content-Type，取值仅限于以下三种
      - text/plain
      - multipart/form-data
      - application/x-www-form-urlencoded
  
  请求流程：
  
  1. 请求头加一个 `Origin` 字段，存储发出该请求的站点的 源（协议、域名、端口号）
  2. 如果 **不允许** 该源跨域，返回一个 **200，但是响应头不包含 `Access-Control-Allow-Origin`**
  3. **允许**，返回 **200，响应头包含 `Access-Control-Allow-Origin`、`Access-Control-Allow-Credentials`、`Access-Control-Expose-Header`**
  
  - `Access-Control-Allow-Origin`：允许发起跨域请求的源列表
  - `Access-Control-Allow-Credentials`：是否允许携带 Cookie，此项为 `true` 的时候，`Access-Control-Allow-Origin` 不能是 `*`，必须指定当前站点的源
  - `Access-Control-Expose-Header`：除去上面的头部信息 
  
- 复杂请求，不满足以上条件的请求都是复杂请求，复杂请求会在正式通信前，增加一次 HTTP 查询请求，称为 预检请求，OPTIONS 方法，来确定服务端是否允许跨域请求



#### postMessage

H5 新增的 **跨文档通信 API**

otherWindow.postMessage(message, targetOrigin, [transfer])

发送信息的一方在自己的脚本内调用 全局变量的 postMessage

- message：要传递的数据
- targetOrigin：目标窗口的 URL
- transfer：一串 Transferable 对象，所有权移交给目标窗口

接收方用 `message` 事件监听来收取其他源用 `postMessage` 传递的信息



#### WebSocket

HTTP 协议是 请求-响应 模型，连接只能由客户端发起，想要维持双工通信就要不停建立连接或者保持连接状态，这要求服务器具有快速的处理速度或者强大的高并发能力

websocket 是 H5 的一个新协议，它允许服务端主动向客户端发送数据，真正实现了双工通信

- 兼容 HTTP 协议，建立连接时借助 HTTP 协议，完成之后的双工通信就和 HTTP 无关了
- 建立在 TCP 协议之上
- 数据格式轻量，通信高效
- 可以发送文本、二进制数据
- 没有同源限制
- 协议标识符是 ws



#### Node 中间件代理

**同源策略是浏览器要遵守的标准，服务器与服务器之间没有同源策略**

- 代理服务器 接收 客户端请求
- 把 请求 转发给 服务器
- 代理服务器拿到 响应
- 把 响应 转发给 客户端



#### Nginx 反向代理

**实现原理类似于 Node 中间件；通过 nginx 配置一个 代理服务器做跳板**



#### window.name + iframe

原理：window.name 这个属性不会因为 url 的变化而变化，只要设置了就一直是这个值；同理，iframe 的 url 变化了，iframe.contentWindow.name 也是不会变的

这样就可以通过转换 iframe 的 url，把要传递的数据存在 name 中，交换回来

假如 A 页面 和 B 页面 同源，C页面有数据要传递给 A 页面

1. A 页面 创建一个 iframe 标签，url 设置为 C 页面
2. C页面 的脚本中把要传递的数据放入 window.name
3. 把 iframe 的 url 改成 B 页面
4. A 页面的脚本 访问 iframe.contentWindow.name



#### location.hash + iframe

hash 改变不会导致页面刷新，所以可以使用

假如 A 页面 和 B 页面 同源，C页面有数据要传递给 A 页面

1. C 页面 创建一个 iframe 标签，url 设置为 B 页面，把 要传递的数据放到 hash 中
2. A 页面创建一个 iframe 标签，url 设置为 C页面，在脚本中设置 hashchange 事件监听，获取 hash 值
3. B 页面是 iframe 的 iframe，且 A/B 同源，那么 A/B 可以 JS 通信，B 页面直接设置，`window.parent.parent.lacation.hash = location.hash`



#### document.domain + iframe

该方法只能用于 二级域名相同的情况下，比如 `a.test.com` 和 `b.test.com`

把这两个页面的 document.domain 都设置为 `test.com` 就行

1. A 页面的 `document.domain` 设置成 `test.com`，创建 iframe 标签，url 设置成 B 页面，添加 iframe 的 load 监听，获取 B 页面的数据
2. B 页面 `document.domain` 设置成 `test.com`



总结

- CORS 支持 所有的 HTTP 请求，是跨域的 根本解决方案
- JSONP 只支持 GET 请求，优点是 支持 IE8/9，以及不支持 CORS 的网站
- Node 中间代理 和 Nginx 反向代理，利用的是 服务器之间没有同源策略
- 日常工作用的多的是 CORS 和 Nginx 反向代理



### CSRF

> 跨站请求伪造（CSRF；Cross-site request forgery）指的是黑客引诱用户打开黑客的网站，在黑客的网站中，利用用户的登录状态发起的跨站请求
>

#### 如何防御

- 验证 HTTP Referer 字段，验证请求的来源地址（也不安全，可篡改）
- 给 Cookie 设置 `samesite` 属性，意为该 cookie 不随着跨域请求发送
- 在请求中加入随机生成的 token（合法站点发送合法请求前随机生成，所以攻击者的站点生成不了），服务端建立拦截器验证 token



### XSS

> 跨站脚本攻击（XSS；Cross Site Scripting） 是指黑客往 HTML 文件中或者 DOM 中注入恶意脚本，从而在用户浏览页面时利用注入的恶意脚本对用户实施攻击的一种手段。
>

影响：

- 盗取 `Cookie` 信息
- 监听用户信息（事件监听）
- 修改 DOM
- 在页面内添加浮窗

注入方式：

- 持久型

  攻击者把恶意代码提交到服务端的资源中去，每次访问该网站都会执行恶意代码

- 反射型

  用户点击了恶意链接，恶意脚本返回到客户端被执行

- 基于 DOM

  通过把恶意脚本注入到 DOM 操作中去，类似反射型

#### 如何防御

- XSS filter
  - 对输入和 URL 参数 进行过滤
  - 对输出进行编码
- CSP（白名单）
  - HTTP 头部
  - HTML 的 meta 标签
- HttpOnly：cookie 不能由 JavaScript 脚本读取



### TCP 如何保证可靠传输

- 校验和
- 序列号、确认应答号
- 重传机制
- 流量控制
- 拥塞控制
- 滑动窗口



### websocket

特点：

- 建立在 TCP 协议之上的应用层协议（wss 建立在 SSL 协议之上）
- 一次握手
- 持久性连接
- 兼容 HTTP 协议，默认端口 80 和 443，握手阶段采用 HTTP 协议
- 数据格式轻量化，通信高效（头部信息很少）
- 可以发文本和二进制数据
- 没有同源策略
- 服务器可以真正主动推送消息

适用于实时更新或者连续数据流，实况直播，即时聊天，实时位置这些

```javascript
ws.onopen = function(event){
    // 建立连接时触发的事件
}
ws.send(msgBody) // 发送数据
ws.close([code], [reason])
ws.onerror = function(event){
    // 报错时触发的事件
    // event.code
    // event.reason
    // event.wasClean 是否
}
ws.onmessage = function(event){
    // 响应触发的事件
}
```



没有 websocket 时的解决办法：创建一个和服务端进行双通道通信的 web 应用，轮询对服务端发送请求

- 服务端被迫维持来自每个客户端大量的连接
- HTTP 头部开销较大，如果每次请求的数据较小，就会很浪费资源



#### 握手头部升级

1. 建立 TCP 连接
2. 如果是 wss 就建立 TLS 连接
3. 发送以下 HTTP 请求进行 websocket 握手（响应状态码101）

```javascript
// 请求头
Upgrade: websocket
Connection: upgrade
Sec-WebSocket-Key: // Bse64 encode 的值，浏览器随机生成，验证 websocket
Sec-WebSocket-Protocol: // 区分同 URL 下，不同服务所需要的子协议
Sec-Websocket-Version: // websocket 的版本
```

```javascript
// 响应头
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: // 服务端确认经过加密的 Key
Sec-WebSocket-Protlcol: // 使用的协议
```



#### 断线重连

断线可能的原因

- websocket 超时没有消息自动断开连接

  解决：心跳包，也就是应用层的保活机制；因为 TCP 的保活机制检查不到机器断电，网线拔出，防火墙这些断线，所以需要在应用层实现一个保活机制

  心跳包可以由客户端发起，也可以由服务端发起

  - 客户端每隔一个时间间隔发送一个心跳包给服务器（一般是空包），此时启动一个超时计时器
  - 服务端接收成功，响应一个包
  - 如果客户端接收到响应包，说明连接正常，删除超时计时器
  - 如果超时还没有收到响应包，说明连接断开



### HTTP 头部

- Connection，连接类型
- Cookie
- Host，域名
- Referer，发送请求的页面的 URI
- user-Agent，浏览器的用户代理字符串



### TCP 报文

明确几个概念：

- 数据链路层传输的数据叫做 **帧（Frame）**
- 网络层（IP）传输的数据叫做 **包（Packet）**
- 传输层（TCP/UDP）传输的数据叫做 **报文（Segment）**



#### TCP

**TCP 是面向字节流的**

比如说现在要传递 100KB 的 HTML文档到另一台主机，并不会整个文档直接发送过去，可能会切割成几个部分，比如 4 个 25KB 的数据段，每个数据段加上一个 TCP 头部，组装成 TCP 报文，然后发送

TCP 报文结构：**TCP 头部 + TCP 数据部分**

TCP 报文头部结构：

- 源端口号 + 目的端口号（各 2 字节）

  用于标志这个报文 **来自哪个应用程序；要送往哪个应用程序（应用程序绑定端口）**

- 序列号（4 字节 0 ~ （2^32 - 1））

  **TCP 面向字节流，在接收方重组的时候就需要知道这些字节的顺序，这就是由序列号表示的**

  序号字段值指的是**本报文段**所发送的数据的第一个字节的序号

  2^32 个字节就是 4GB 的数据，这个范围基本上不会出现 一个TCP 报文段中 两个字节的序列号冲突的情况

- 确认应答号（4 字节 0 ~ （2^32 - 1））

  TCP 是标准的 **请求-响应模型**，每个报文都有它对应的确认应答报文（ACK 报文）确认应答号就是希望收到的下一个序列号

- 数据偏移（4 位）

  TCP 数据部分的起始位置，因为一个 TCP 报文有头部和数据部分，接收方要组装的是 数据部分，所以就靠这个字段来定位

- 标志位（6 位）

  - URG：紧急指针有效，这个报文应当尽快传送

  - ACK：这是个 **确认应答报文，确认应答号字段生效；建立连接后的所有报文的 ACK 字段都必须是 1**
  - PSH：该报文段应当尽快推送给接收应用程序
  - RST：复位字段，表示当前连接出错
  - SYN：这是个 **请求连接报文**
  - FIN：这是个 **请求断开连接报文**

- 窗口大小（2 字节）

- 校验和（2 字节）检验 TCP 报文完整性

- 紧急指针（2 字节）

  URG = 1 时生效，**本报文段中紧急数据的字节数，也就是 本报文段最前面的这些字节是紧急数据，剩下的是普通数据**



#### TCP 如何保证可靠性

- 校验和，保证 TCP 报文的完整性，有损就重传

- 序列号、确认应答号，保证 TCP 报文段有序、无重复的到达接收方

- 重传机制，保证 TCP 报文一定能到达接收方

  - 超时重传
  - 快速重传

- 滑动窗口

  TCP 每发送一个报文都要接收到应答之后才能发送下一个，这样的话，RTT越大，通信效率越低；

  所以 TCP 引入了滑动窗口，当窗口没有关闭的时候，即使上一个报文的应答还未接收到，也可以发送下一个 TCP 报文；

- 流量控制，根据接收方的数据处理能力，控制发送方发送的数据量，防止频繁触发重传。

- 拥塞控制，根据网络的拥堵情况，调整发送方发送的数据量，防止频繁触发重传



### HTTP /1.0 对比 HTTP/1.1

| 对比点                | 1.0                                      | 1.1                                             |
| --------------------- | ---------------------------------------- | ----------------------------------------------- |
| 请求方法              | `GET/POST/HEAD`                          | 新增了 `PUT/DELETE/OPTIONS/CONNECT/PATCH/TRACE` |
| 连接方式              | 默认短连接（可以设置长连接）             | 默认长连接                                      |
| 头部信息              | 没有 `Host` 字段                         | 新增了 `Host` 字段                              |
| 断点续传              | 不支持                                   | 头部新增了 `range` 域，支持                     |
| HTTP 缓存（头部字段） | `If-Modified-Since/Expires/Catch-Contol` | 新增了 `Etag/If-None-Match`                     |

共同的缺陷：

- 队头阻塞，虽然 HTTP/1.1 使用了长连接，但是仍然没有改善队头阻塞的问题，因为 **HTTP 请求是串行的，同一时刻只能处理一个请求**
- 无状态，每次 HTTP 请求（响应），都需要带上头部信息，这部分不容忽视
- 明文传输，二者都采用明文传输



如何优化：

- 避免发送 HTTP 请求，使用 HTTP 缓存
- 减少 HTTP 请求次数
  - 减少重定向
  - 合并请求，小资源合并成大资源
  - 延迟（按需）发送请求
- 减少 HTTP 响应的数据大小，压缩



### HTTP/2 的优化

- 头部压缩

  采用 `HPACK` 算法，双端共同维护一张 **头部信息表**，每次请求和响应只传送更新的头部信息

- 二进制格式

  `HTTP/1.x` 的报文是文本格式的，这在传输之前需要转换成二进制格式，所以 HTTP/2.0 直接省略了这一步，直接采用 二进制格式

- 多路复用

  - HTTP/2.0 **同个域名只建立一个 TCP 连接**
  - 每个请求（响应）称为一个 **流（Stream）**
  - HTTP/2.0 会把每个流打散成一个个的 **帧（Frame）**
  - **乱序的帧组成一个消息，交由 TCP 层 包装成 TCP 报文发送**
  - 这样 **并行，交错的发送多个请求、响应**，一定程度上解决了 **1.x 的队头阻塞问题**
  - **每个流都有优先级编号，客户端和服务端可以根据此采取对应的处理策略（客户端是奇数，服务端是偶数）**

- 服务器推送

  2.0 在一定程度上改善了 请求-应答模式，允许服务器推测客户端接下来需要的 **静态资源**，主动推送



### HTTP/2 的问题

- TCP 以及 TCP + TLS 建立连接的延时

  TCP 握手 1.5 RTT；TLS/1.2 2 RTT；TLS/1.3 1.5 RTT

  为了确保可靠传输，握手过程不可避免

- HTTP 的队头阻塞解决了；TCP 的阻塞仍然存在

  一个 TCP 连接即便有滑动窗口，触发重传的时候仍然会阻塞后面的所有报文 

- 多路复用导致服务器压力上升，很多请求并行的话，会导致服务器的 QPS（每秒请求数）

- 多路复用容易让请求超时



### HTTP/3

- 传输层使用 UDP 协议
- 3 优化了多路复用，Stream 之间是独立的，当其中一个流发生了丢包，只会阻塞这个 Stream
- 3 不再使用四元组唯一确定连接，而是通过 客户端生成的一个 Connection ID（64 位）唯一确定一个连接
- 还有很多