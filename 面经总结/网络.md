## HTTP

------

### HTTP常见面试题

------

#### HTTP基本概念

------

超文本传输协议

> HTTP 是一个在计算机中专门在 **两端** 之间传输 文本、图片、音频、视频等 **超文本** 数据的 **约定和规范**

主要从三方面来理解

1. 超文本：字面意思，就是能够传输的数据不只有文本，现在文本的含义已经可以扩展为图片、视频、压缩包等了，**超的关键是超链接**，用于从一个超文本跳转到另一个超文本
2. 传输：端对端进行数据搬运，两个特点：双向，可中转
3. 协议：某个行为的两个以上的参与者用来制约大家行为的约定和规范



#### HTTP常见状态码

------

|      | 具体含义                                           | 常见状态码      |
| ---- | -------------------------------------------------- | --------------- |
| 1xx  | 提示信息，表示目前处于协议处理的中间状态           |                 |
| 2xx  | 成功，已经收到报文并被正确处理                     | 200/204/206     |
| 3xx  | 重定向，资源位置发生变动，需要重新向服务器发起请求 | 301/302/304     |
| 4xx  | 客户端错误，请求报文出错                           | 400/403/404     |
| 5xx  | 服务端错误，服务器在处理请求时内部出错             | 500/501/502/503 |

- `2xx` 成功
- `200 OK`：一切正常，除了 `HEAD` 请求，都会有 `body` 数据
- `204 No Content`：与 200 基本相同，没有 `body` 数据
- `206 Partial Content`：HTTP分块下载或断点续传，表明当前返回的 `body` 数据不是资源的全部
- `3xx` 重定向
- `301 Moved Permanently`：永久重定向
- `302 Found`：临时重定向，301和302都会在响应头里使用字段 `Location`，指明后续要跳转的 URL
- `303 See Other`：重定向到其他资源，常用于 POST/PUT 方法的响应
- `304 Not Modified`：协商缓存命中，这代表服务器端数据没有更新，浏览器直接使用缓存中的数据
- `307 Temporary Redirect`：类似 `302` 的临时重定向，请求方法不得更改
- `308 Permanent Redirect`：类似 `301` 的永久重定向，请求方法不得更改
- `4xx` 客户端发送的报文有误
- `400 Bad Request`：客户端请求的报文有错误
- `401`：未授权，需要登录
- `403 Forbidden`：服务器禁止访问资源，请求没错
- `404 Not Found`：请求的资源在服务器未找到
- `405`：禁用请求中指定的方法
- `5xx` 服务器处理请求时内部出错
- `500 Internal Server Error`：服务器出错
- `501 Not Implemented`：客户端请求的功能还未实现
- `502 Bad Gateway`：通常是服务器作为⽹关或代理时返回的错误码，表示服务器⾃身⼯作正常，访问后端服务器 发⽣了错误
- `503 Service Unavailable`：服务器繁忙



#### HTTP 常见字段

| 字段名           | 作用                                                         |
| ---------------- | ------------------------------------------------------------ |
| Host             | 指定服务器的域名                                             |
| Content-length   | 本次相应的数据长度                                           |
| Connection       | 常⽤于客户端要求服务器使⽤ TCP 持久连接(keep-alive)，直到客户端或服务器主动关闭连接 |
| Content-Type     | ⽤于服务器回应时，告诉客户端，本次数据是什么格式             |
| Accept           | 客户端请求的时候，⽤以声明⾃⼰可以接受哪些数据格式           |
| Content-Encoding | 表示服务器返回的数据使⽤了什么压缩格式                       |
| Accept-Encoding  | 客户端在请求时，⽤以说明⾃⼰可以接受哪些压缩⽅法             |



#### GET 与 POST 的区别？

GET 方法用于从服务器获取资源

POST 方法用于对服务器的资源进行修改



#### GET 和 POST 方法 是 安全和幂等的吗？

> 安全：在 HTTP 协议中，是指请求方法不会 **破坏** 服务器上的资源

> 幂等：多次 **执行相同的操作**，**结果都是相同的**

- GET
  - 因为 GET 方法是 **只读** 操作，所以肯定 **是安全的**
  - 一般意义上来讲，GET 方法 **是幂等的**，因为 GET方法是用来从服务器获取资源的只读操作，它不应该改变服务器上的资源
- POST
  - POST 方法是 **新增或提交数据的** 操作，会修改服务器上的资源，所以 **不是安全的**
  - 每一次提交都会修改服务器上的资源，所以执行相同的操作，结果也不同，**不是幂等的**



#### HTTP/1.1 的优点有哪些，怎么体现的？

1. 简单：HTTP 基本的报文格式是 `header + body`，头部信息也是 `key-value`形式的简单文本，易于学习
2. 灵活和易于扩展：请求方法多种多样；URL/URI 除了少数几个参数是固定的，大部分是自定义的；状态码除了常用的还可以自己定义；头部信息也是可以自定义；等等，HTTP协议中很多都允许开发人员 **自定义和扩充**；HTTP 位于 应用层，下层可以随意变化；比如 HTTPS 就是在 HTTP 和 TCP 之间建立了 SSL/TLS 安全传输层；HTTP3 甚至把 TCP 换成了基于 UDP 的 QUIC
3. 应用广泛和跨平台：各种用户端都在使用，天然跨平台

#### HTTP/1.1  的缺点有哪些？

- 无状态
  - 好处：不需要额外的资源记录状态信息，减轻服务器压力
  - 坏处：服务器没有记忆，在完成有关联性的操作的时候会非常麻烦；
  - 坏处的解决方案之一：Cookie，通过在请求和响应报文中写入 Cookie 来控制客户端的状态
- 明文传输
  - 好处：方便阅读，浏览器的 F12 控制台或 Wireshark 抓包都可以直接查看
  - 坏处：HTTP 的所有信息都是暴露下的，多阶段的传输可能会导致信息泄露
- 不安全（**缺点**）
  - 明文传输，信息可能会泄露
  - 不验证通信方的身份，可能遭遇伪装
  - 无法验证报文的完整性，有可能已经被篡改了

**安全问题如何解决？**

HTTPS，通过引入 SSL/TLS 安全传输层来解决

#### HTTP/1.1 的性能如何？

HTTP 协议基于 **TCP/IP**，使用 **请求-应答** 的通信模式

- 长连接（TCP 持久连接）

一般的 HTTP 请求（短连接），是**每发起一次请求都要进行一次 TCP 连接（三次握手），而且是串行请求**，这无疑会增加通信开销

长连接就是解决这个问题的，它有效的**减少 TCP 连接的重复建立和断开造成的额外开销**，减轻了服务器端的压力

特点是：**只要任意一端没有明确断开连接，就保持 TCP 连接的状态**

- 管道网络传输

长连接使得管道网络传输成为了可能

短连接是这样的：

1. TCP 建立连接
2. 请求A发送
3. 请求A响应
4. TCP 断开连接
5. TCP 建立连接
6. 请求B发送
7. 请求B响应
8. TCP 断开连接

B请求发送之前必须等 A 请求完成响应，并且断开TCP连接，然后重连

长连接的普通形式：

1. TCP 建立连接
2. 请求A发送
3. 请求A响应
4. 请求B发送
5. 请求B响应
6. TCP 断开连接

这样的 长连接 解决的问题是 TCP 断开重连的性能额外开销，但是，**请求B的发送仍旧需要等待 请求A 的响应完成**

**管道机制**

1. TCP 建立连接
2. 请求A发送
3. 请求B发送
4. 请求A响应
5. 请求B响应
6. TCP 断开连接

请求 A 和 请求 B 可以同时发送，**但是响应依旧是按照请求发送顺序的**

要是前⾯的回应特别慢，后⾯就会有许多请求排队等着。这称为 **队头堵塞**。

- 队头阻塞

**请求 - 应答** 的模式加剧了 HTTP 的性能问题。 因为当顺序发送的请求序列中的⼀个请求因为某种原因被阻塞时，在后⾯排队的所有请求也⼀同被阻塞了，会招致 客户端⼀直请求不到数据，这也就是 **队头阻塞**。好⽐上班的路上塞⻋



#### HTTP 与 HTTPS 的区别？

1. HTTP 是明文传输，有安全风险，HTTPS 在 HTTP 和 TCP 之间加入了 SSL/TLS 安全协议，使得报文能够加密传输
2. HTTP 连接建立相对简单，TCP 三次握手之后就可以进行 HTTP 的报文传输；HTTPS 在 TCP 三次握手之后，还需要进行 SSL/TLS 的握手过程，才能进行加密报文的传输
3. HTTP 的端口号是 80；HTTPS 的端口号是 443
4. HTTPS 协议需要向 证书权威机构（CA）申请数字证书，来保证服务器的身份是可信的



#### HTTPS 解决了 HTTP 的哪些问题？

1. 窃听风险：因为是明文传输而导致的
2. 篡改风险：因为无法验证报文完整性（也是因为明文传输）
3. 冒充风险：无法验证通信方身份

加入 SSL/TLS 协议之后，解决了以上问题：

1. 信息加密：窃听到信息之后，也没办法解密
2. 校验机制：通信内容篡改之后就无法正常显示
3. 身份验证：解决冒充问题



#### HTTPS 是如何解决 HTTP 的问题的？

1. **混合加密** 实现信息的 **机密性**，解决窃听问题

> 混合加密就是结合使用 对称加密 和 非对称加密

- 在通信建立前采用 **非对称加密** 的方式交换 **会话秘钥**
- 通信过程中使用 **对称加密** 的方式进行加/解密明文数据

为什么采用混合加密？

- 对称加密使用一个秘钥，**运算速度快**，但是 **无法做到安全的密钥交换**
- 非对称加密使用 公钥和私钥，**解决了密钥交换问题但是速度慢**

2. **摘要算法** 实现数据的 **完整性**，防止数据被篡改

客户端发送的加密数据包含两部分：明文 和 摘要

服务端解密后，用相同的摘要算法计算获得的明文，再把结果和接收的摘要进行比较，如果相同则说明数据完整

3. 把服务器公钥放入到 **数字证书**，解决冒充风险

数字证书包含 **服务器公钥 和 CA 数字签名**

客户端通过 CA的公钥检验数字证书的真实性，如果是真实的（说明通信方的身份是有效的），就使用 数字证书中的服务器公钥加密报文，这样就解决了冒充的风险



#### HTTPS 是如何建立连接的？SSL/TLS1.2 四次握手阶段交互了什么？

SSL/TLS 协议基本流程：

- 客户端向服务器索要并验证服务器的公钥
- 双方协商生成 **会话秘钥**
- 双方使用会话秘钥进行加密通信

前两步就是 SSL/TLS 的建立过程（握手阶段）

SSL/TLS 的握手阶段涉及四次通信

1. Client Hello

   由客户端向服务器发起加密通信请求，也就是 `Client Hello` 请求

   主要包含以下信息：
   1. 客户端支持的 SSL/TLS 协议版本
   2. 客户端生成的 第一个**随机数（Client Random）**，用于生成 **会话秘钥**
   3. 客户端支持的加密方法列表

2. Server Hello

   服务端收到 `Client Hello` 请求后，向客户端发出响应 ` Server Hello`

   主要内容如下：

   1. 确认的 SSL/TLS 协议版本，如果浏览器不支持，则关闭加密通信
   2. 服务端生成的 第二个 **随机数（Server Random）**，用于生成 **会话秘钥**
   3. 确认的加密方法列表
   4. 服务器的数字证书

3. 客户端回应

   客户端接收到 `Server Hello` 之后，首先通过 CA 公钥，确认数字证书的真实性；如果数字证书确认无疑，客户端会从数字证书中获得服务器公钥，用于加密以下报文

   1. 第三个 **随机数（pre-master key）**，依旧用于生成 **会话秘钥**
   2. **加密通信算法改变通知**，这次之后客户端发起的通信都是使用 **会话秘钥** 对称加密的
   3. **客户端握手结束通知**，以及之前所有内容的发生的数据做个摘要，供服务端校验

   **三个随机数**，使用双方商定的**加密算法**，**各自生成** 之后通信用的 **会话秘钥**

4. 服务端最后的回应

   服务器端收到客户端回应的密文，使用 **私钥** 解密，得知第三个随机数，使用加密算法，算出 **会话秘钥**，然后向客户端发送最后信息：

   1. **加密通信算法改变通知**，表示之后的响应信息都是使用 **会话秘钥** 对称加密的
   2. **服务器握手结束通知**，以及之前所有内容的发生的数据做个摘要，供客户端校验

至此，SSL/TLS 握手阶段结束。接下来的通信都是使用 **会话秘钥** 进行加密的



#### HTTP/1.1 相比 HTTP/1.0 性能上的改进

- TCP 长连接 改善了 短连接的时候，TCP连接频繁断开重连造成的性能开销
- TCP 长连接使得管道网络传输得以实现，第一个请求发出，不必等待响应就可以发出第二个请求，一定程度上减少了整体的响应时间



#### HTTP/1.1 的性能瓶颈

- 请求/响应头部未经压缩就发送，头部信息越多，延迟越大，只能压缩 body 部分
- 每次互相发送相同的头部信息会造成额外的性能浪费
- 队头阻塞，因为服务端是按照请求顺序进行相应的，如果某个响应很慢，后面的响应都会阻塞
- 没有请求优先级控制
- 请求只能从客户端开始，服务端只能被动响应



#### HTTP/2 对 HTTP/1.1 的性能瓶颈做出了什么优化？

HTTP/2 是基于 HTTPS 的，所以 HTTP/2 的安全性是有保障的，而且HTTP/1.1之后，所有通信默认使用长连接

改进：

- 头部压缩

  如果同时发出多个请求，头部一样或者相似，HTTP/2 **会消除重复的部分**

  也就是 `HPACK` 算法：客户端和服务端同时维护一张头信息表，所有字段存入这张表中，生成一个索引号，同样字段不再发送，只发送索引号，这样速度就提高了

- 二进制格式

  HTTP/1.1 的报文是文本形式的，HTTP/2 使用 **二进制**，头信息和数据体都是二进制，统称为帧（frame）：头信息帧和数据帧。这样省略了服务端文本转二进制的过程，**增加了数据传输效率**

- 数据流

  HTTP/2 的数据包也不是按照顺序发送的了，**同一个连接 中连续的数据包，可能属于不同的请求（响应）**，因此需要对数据包做标记，指出它属于谁

  **一个请求或响应的所有数据包，统称为一个数据流（Stream）**，每个数据流都有一个唯一编号；客户端发出的数据流编号为奇数，服务端发出的为偶数

  客户端还可以 **指定数据流的优先级，高优先级先响应**

- 多路复用

  HTTP/2 可以 **在一个连接中并发多个请求或相应，不用按照顺序一一对应**

  移除了 HTTP/1.1 的串行请求，就不会有 **队头阻塞** 的问题了，降级了延迟，大幅提高了连接利用率

  举例来说，在⼀个 TCP 连接⾥，服务器收到了客户端 A 和 B 的两个请求，如果发现 A 处理过程⾮常耗时，于是就 回应 A 请求已经处理好的部分，接着回应 B 请求，完成后，再回应 A 请求剩下的部分。

- 服务器推送

  HTTP/2 在一定程度上改善了传统的 **请求—响应** 工作模式，**服务端不再只能被动响应，也可以主动向客户端发送消息**

  举例来说，在浏览器刚请求 HTML 的时候，就提前把可能会⽤到的 JS、CSS ⽂件等静态资源主动发给客户端，减 少延时的等待，也就是服务器推送（Server Push，也叫 Cache Push）



#### HTTP/2 的缺陷？HTTP/3 做了哪些优化？

HTTP/2 主要的问题在于，多个 HTTP 请求复用同一个 TCP 连接，这对于 TCP协议来说是无感的，一旦发生了丢包现象，就会触发 TCP 重传，**这样在一个 TCP 连接中的所有 HTTP 请求都必须等待这个丢了的包被重传回来**

- HTTP/1.1 中的管道网络传输会有队头阻塞的问题
- HTTP/2 多个请求复⽤⼀个TCP连接，⼀旦发⽣丢包，就会阻塞住所有的 HTTP 请求

这是 TCP 传输层的问题，**所以 HTTP/3 就把 TCP协议 改为了 UDP**

UDP 不管顺序，也不管丢包，所以不会出现 HTTP/1.1 的队头阻塞和 HTTP/2 的一个丢包全部重传问题

**UDP 是不可靠传输，但基于 UDP 的 QUIC 协议可以实现类似 TCP 的可靠性传输**

- 当某个流丢包时，只会阻塞这个流，其他流不受影响
- TLS3 升级为了 1.3 版本，头部压缩算法升级为了 `QPACK`
- HTTPS 建立连接，TCP 三次握手，TLS/1.3 三次握手。QUIC 将这六次交互合并成了 3 次，减少了交互次数

所以， QUIC 是⼀个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复⽤的协议



### HTTP/1.1 如何优化？

------

长连接，这个是 HTTP/1.1 的默认选项，从传输层入手，减少无效的 TCP 连接断开重连，来减轻服务器的压力，减少网络传输的延迟

除此之外，还有三个方向：

- 尽量避免发送 HTTP 请求
- 必须发送 HTTP 的请求时，考虑如何减少次数
- 减少服务器的 HTTP 响应的数据大小



#### 如何避免发送 HTTP 请求？

缓存，对于一定时间内可能重复请求，并且数据在一定时间内可能不会改变的请求，就应该缓存到本地（磁盘或内存）

客户端会把第一次请求以及响应的数据保存在本地，URL 为 key，响应作为 value

后续发起相同请求时，就先在缓存中查找该请求的相应数据，找到，直接从本地读取该响应。

缓存下来的数据包含头部信息，其中会有该响应缓存的过期时间，如果发现缓存的数据是过期的，就会重新发送请求。

客户端重新发送请求时，在请求的头部信息中，有一个 `ETag`，它的值是缓存的响应中的摘要（唯一标识响应的资源），服务端收到请求之后，会把这个摘要和服务端资源的摘要进行对比：

- 如果不同，说明客户端的缓存失效了，服务端会在响应中带上最新资源
- 如果相同，说明客户端缓存依旧是最新的，那就返回 `不含 body 的 304 Not Modified 响应`



#### 如何减少 HTTP 请求次数？

- 减少重定向请求次数

  - 每一次重定向就意味着多进行一次 HTTP 请求，这无疑会降低网络的性能

  - 服务端往往不止有一台服务器，比如源服务器上一级是代理服务器，与客户端直接通信的是代理服务器，那每次 请求-响应，都会重复两遍，如下

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url1` 请求
    3. 源服务器 —> 代理服务器，返回 `/url1` 响应（302 Found; Location: /url2）
    4. 代理服务器 —> 客户端，返回 `/url1` 响应（302 Found; Location: /url2）
    5. 客户端 —> 代理服务器，发送 `/url2` 请求
    6. 代理服务器 —> 源服务器，发送 `/url2` 请求
    7. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    8. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

    **如果能让 代理服务器 完成重定向的工作，那就能减少 HTTP 请求的次数**

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url1` 请求
    3. 源服务器 —> 代理服务器，返回 `/url1` 响应（302 Found; Location: /url2）
    4. 代理服务器 —> 源服务器，发送 `/url2` 请求
    5. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    6. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

    **再进一步，如果 代理服务器 不用与 源服务器 沟通，就能知道这个请求要重定向，直接请求重定向 URL，那又减少了 HTTP 请求的次数**

    1. 客户端 —> 代理服务器，发送 `/url1` 请求
    2. 代理服务器 —> 源服务器，发送 `/url2` 请求
    3. 源服务器 —> 代理服务器，返回 `/url2` 响应（200 OK; 资源）
    4. 代理服务器 —> 客户端，返回 `/url1` 响应（200 OK; 资源）

- 合并请求

  如果把多个小数据量的请求合并为一个请求，虽然传输资源总量不变，但是请求次数减少，就等同于 **减少了重复发送的 HTTP 头部（目的等同于 HTTP/2 的头部压缩）**

  除此之外，**合并请求也能预防队头阻塞的问题**

  合并请求有以下几种方法：

  1. `CSS Image Sprites` 把多个小图片、小图标合并为一个大图片，一次性请求返回，然后再根据 CSS 数据把大图片切割成多张小图片（减少了 HTTP 请求次数）
  2. 服务端使用 `webpack` 等打包工具把 `JS/CSS` 等资源打包成大文件（减少了 HTTP 请求次数）
  3. 还可以把图片的二进制数据用 `base64` 编码后，以 URL 的形式嵌入到 HTML 文件，跟随 HTML 文件一起发送（这样客户端收到 HTML 后，就可以直接解码出数据，然后直接显示图⽚，就不⽤再发起图⽚相关的请求，这样便减少了请求的次数）

  合并请求就是为了合并资源，以一个大资源的请求替换多个小资源的请求

  **问题：如果其中某个小资源发生了变化，那么整体都要重新请求，这就会带来额外的性能开销**

- 延迟发送请求

  每次都请求用户看到的页面资源，当用户有查看新资源的需求的时候，在再向服务端请求新的资源，这样就做到了延迟发送请求的效果



#### 如何减少 HTTP 响应的数据大小？

- 无损压缩

  > 资源经过压缩后，信息不被破坏，还能完全恢复到压缩前的原样

  适合文本文件、程序可执行文件、程序源代码

  常见的有：gzip，deflate，br（效率比 gzip 高）

  `Accept-Encoding` 请求头就是客户端告诉服务端，客户端支持的压缩算法

  `content-encoding` 响应头就是服务端告诉客户端该资源使用的压缩算法

- 有损压缩

  > 主要将次要的数据舍弃，牺牲⼀些质量来减少数据量、提⾼压缩⽐；解压的数据会与原始数据不同但是⾮常接近

  常用于多媒体数据，比如音频、视频、图片

  可以通过 HTTP 请求头部中的 `Accept` 字段⾥的 `q 质量因⼦`，告诉服务器期望的资源质量

  对于图片的压缩，目前压缩比较高的是 **Google 的 WebP 格式**

  对于音视频的压缩，⾳视频主要是动态的，每个帧都有时序的关系，通常时间连续的帧之间的变化是很⼩的。

  ⽐如，⼀个在看书的视频，画⾯通常只有⼈物的⼿和书桌上的书是会有变化的，⽽其他地⽅通常都是静态的，于是 只需要在⼀个静态的关键帧，使⽤增量数据来表达后续的帧，这样便减少了很多数据，提⾼了⽹络传输的性能。对 于视频常⻅的编码格式有 H264、H265 等，⾳频常⻅的编码格式有 AAC、AC3



### RSA 握手解析

------

HTTPS 是在 HTTP 和 TCP 之间添加了 SSL/TLS 安全传输层，来解决 HTTP 的一些信息安全问题

SSL/TLS 1.2 的连接需要进行四次握手，2 个 RTT（Round-Trip Time，往返时延），SSL/TLS 1.3 对此进行了优化，只需要进行三次握手 也就是 1 个 RTT

HTTPS 进行 SSL/TLS 握手的时候采用的是混合加密模式，建立连接的时候使用非对称加密通信，交换秘钥，然后以相同的加密算法合成出 对称加密使用的 会话秘钥

不同的密钥交换算法，TLS 握手过程会有一些区别

这部分就是通过一个具体的密钥交换算法：RSA，去观察 TLS 握手的一些细节

#### RSA 握手过程

传统 TLS 握手基本都是使用 RSA 算法实现密钥交换的，在将 TLS 协议部署到服务端时，证书文件包含一对公私钥，公钥会在 TLS 握手阶段传递给客户端，私钥服务端使用

RSA 秘钥协商算法中，客户端会生成随机秘钥，并使用服务端的公钥加密后再传给服务端，公钥加密的信息，只能由对应私钥解密

##### TLS 第一次握手

客户端发送一个 **Client Hello** 消息

三部分信息：

- 客户端使用的 SSL/TLS 版本
- **Client Random** 随机数
- 客户端支持的加密方法列表

##### TLS 第二次握手

服务端接收到 **Client Hello** 消息，向客户端发送 **Server Hello** 消息

- 服务端确认支持的 SSL/TLS 版本
- **Server Random** 随机数
- 服务端选择的加密方法

加密方法比如 `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256(0xc02b)`

加密方法的基本形式是 **密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法**

WITH 前面两个单词，第一个是 约定密钥交换的算法，第二个是 约定证书的验证算法（如果只有一个，说明 两个算法都是用这个）

- 握手后的通信使用 AES 对称算法，秘钥长度 128 位，分组模式是 GCM
- 摘要算法 SHA256 用于消息认证和产生随机数

验证服务端身份是通过 **Server Certificate** 消息，其中包含了该服务端的数字证书

然后是 **Server Hello Done** 消息，表示此次应答结束

##### 客户端验证证书

数字证书一般包含如下信息

- 服务端的公钥
- 持有者信息
- CA 的信息
- CA 的数字签名以及使用的算法
- 证书有效期
- 额外信息

数字证书包含两部分内容：

1. 持有者的公钥，用途，颁发者，有效时间等信息，通过 Hash 计算得出的 Hash 值
2. CA 用自己的私钥把 Hash 值加密后，生成的 **数字签名**

客户端校验证书：

1. 客户端使用同样的 Hash 算法获取证书上的 Hash 值 H1
2. 客户端使用 CA 的公钥解密数字签名，获得另一个 Hash 值 H2
3. 比较 H1 和 H2，相同就是有效证书



**证书信任链**

证书一般是有多层的，跨层级的证书之间是没有信任可言的，每一层证书都由它的上一级签发方认证，客户端一般只信任 根证书，根证书信任它的下一级证书，这样就形成了 **证书信任链**，**证书只能由颁发者验证是否可信**

这样做是为了确保根证书的绝对安全性，这样某一层级的证书失效了，那上层的证书可能依旧是可信的



##### TLS 第三次握手

客户端验证完证书之后，就会生成一个新的随机数 **pre-master**，用服务器的公钥加密该随机数，通过 **Change Cipher Key Exchange** 消息传给服务端

服务端收到后，用私钥解密，获得 **pre-master**

至此，双方都得到了 三个随机数 **Client Random, Server Random, pre-master**，然后用 约定好的对称加密算法，生成会话秘钥，用于之后 HTTP 请求的加解密

生成完会话秘钥后，客户端会发送一个 **Change Cipher Spec**，通知服务端开始使用加密方式通信

然后，客户端再发送一个 **Encrypted Handshake Message（Finishd）** 消息，把之前所有发送的数据做个摘要，再用会话秘钥加密一下，是为了让服务端验证加密通信是否可用以及之前的握手信息是否被篡改过

**Change Ciper Spec 消息之前传输的信息，都是明文，之后的都是对称加密的密文**



##### TLS 第四次握手

服务器也是发送 **Change Cipher Spec 和 Encrypted Handshake Message** 消息，如果双方验证加密和解密没问题，那么握手完成，之后使用会话秘钥加解密 HTTP 请求



##### RSA 的缺陷

**不支持前向保密**：前两个随机数是明文传输，第三个随机数是公钥加密的，如果服务端的私钥泄露了，随机数就会被解密

DH 秘钥协商算法解决了这一问题



### ECDHE 握手解析

****

####  离散对数

DH 算法的数学基础

> (a^i) % p = b

- a 和 p 是公共参数，即公开的
- 如果我们知道 i 是多少，很容易算出 b 是多少
- 但是如果我们知道 b 是多少，几乎无法算出 i 是多少



#### DH 算法

假设 A 和 B 使用 DH 算法交换秘钥，那么基于离散对数，底数和模数是公开的，我们用 P 和 G 来代称

然后 A 和 B 各自生成自己的 私钥，A 的私钥记为 a，B 的私钥记为 b

现在二者都有 P 和 G，以及各自的私钥，那就可以计算出各自的公钥：

- A 的公钥记为 **a`**，**a`** = (G^a) % P
- B 的公钥记为 **b`**，**b`** = (G^b) % P

双方交换公钥，A 手上有五个参数，a, **a`**, G, P, **b`**；B 手上也有五个参数，b, **b`**, G, P, **a`**

因为

(**a`** ^ b) % P = (((G ^ a) % p) ^ b) % P = (G ^ (a * b)) % P

(**b`** ^ a) % P = (((G ^ b) % P) ^ a) % P = (G ^ (a * b)) % P

所以 A 和 B 双方可以使用自己手中的参数，算出一个相同的数，我们称之为 K

K 就可以用来当做 **对称加密秘钥**

其中 G，P，**a`**，**b`** 是公开的，无法破译出 a 或者 b 就无法算出 对称加密秘钥，虽然理论上可以通过离散对数的逆运算算出 a 或者 b，但是几乎不可能，因此 DH 算法是安全的



#### DHE 算法

根据私钥生成的方式，DH 算法分为两种具体实现：

- static DH 算法，已废弃

  static DH 算法中，一方（通常是服务端）的 私钥是不变的，另一方的私钥是 每次 TLS 连接实时生成的。

  私钥固定，意味着公钥也是不变的，有了大量的秘钥协商过程的数据之后，就可以暴力破解出固定的私钥了，所以 static DH 算法不具备前向安全性

- DHE 算法，现在常用

  DHE 算法就是双方的私钥每次 TLS 连接都是动态生成的，那即使某一次连接的私钥被破解了，其他的通信过程依然是安全的。**每个通信过程 的私钥都是没有任何关系的，都是独⽴的，这样就保证了前向安全**



#### ECDHE 算法

DHE 算法由于计算量较大，所以性能欠佳，ECDHE 解决了 DHE 的性能问题

简单来说，就是优化了 公钥的计算过程

- 双⽅事先确定好使⽤哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；
- 双⽅各⾃随机⽣成⼀个随机数作为私钥d，并与基点 G相乘得到公钥Q（Q = dG），此时⼩红的公私钥为 Q1 和 d1，⼩明的公私钥为 Q2 和 d2；
- 双⽅交换各⾃的公钥，最后⼩红计算点（x1，y1） = d1Q2，⼩明计算点（x2，y2） = d2Q1，由于椭圆曲线 上是可以满⾜乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 ，因此双⽅的 x 坐标是⼀样的，所以 它是共享密钥，也就是会话密钥。

#### ECDHE 握手过程

**RSA 必须完成 四次握手才能传输应用数据** 

**ECDHE 在第四次握手之前，客户端就已经发送了加密的 HTTP 数据**，这样提高了传输的效率



##### TLS 第一次握手

客户端发送 **Client Hello** 消息，包干如下信息

- 客户端使用的 SSL/TLS 版本号
- 客户端生成的随机数 **Client Random**
- 客户端支持的加密套件列表



##### TLS 第二次握手

服务端发送 **Server Hello** 消息，包含如下信息

- 服务端确认的 SSL/TLS 版本号
- 服务端生成的随机数 **Server Random**
- 服务端选择的加密套件

加密套件比如：`TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384`

- 秘钥协商算法使用 ECDHE
- 签名算法使用 RSA
- 握手后的通信使用 AES 对称算法，秘钥长度 256 位，分组模式是 GCM
- 摘要算法使用 SHA384

然后服务端发送 **Certificate** 消息，包含服务端的数字证书

然后就是与 RSA 握手过程不同的了，**服务端发送 Server Key Exchange** 消息，做了下面几件事

- 选择 椭圆曲线，目的是确定 **基点 G**
- **生成随机数作为服务端的私钥**，保留在本地
- 根据 基点 G 和 私钥计算出**服务端的椭圆曲线公钥**

选择的椭圆曲线 和 服务端的椭圆曲线公钥会随着消息发送给客户端

为了保证这个椭圆曲线的公钥不被第三⽅篡改，**服务端会⽤ RSA 签名算法给服务端的椭圆曲线公钥做个签名**

然后就是 **Server Hello Done** 消息



##### TLS 第三次握手

首先还是验证数字证书是否合法，走证书信任链，逐级验证证书，确认服务端身份

然后**客户端生成随机数作为客户端的私钥**，再根据 基点 G 和私钥 **计算出客户端的椭圆曲线公钥**，使用 **Client Key Exchange** 消息把客户端公钥发送给服务端

然后双方就都持有自己的私钥，对方的公钥，椭圆曲线基点 G，然后就可以计算出点坐标，其中 x 坐标双方相同，理论上 **x 已经可以作为 对称加密的秘钥了**，但是实际中，需要使用 **Client Random, Server Random, x** 三者使用 对称算法，算出最终的会话秘钥

因为计算机中不存在真正的随机数，都是伪随机，三个伪随机数合成一个随机数，更随机一些

然后客户端发送 **Change Cipher Spec** 消息，通知服务端之后使用对称加密通信

**Encrypted Handshake Message** 消息，把之前的数据做个摘要，再用对称秘钥加密，发送给服务端做验证



##### TLS 第四次握手

最后，服务端也会有⼀个同样的操作，发 

Change Cipher Spec 和 Encrypted Handshake Message 消息，如果双⽅都验证加密和解密没问题，那么握⼿正式完成。于是，就可以正常收发加密的 HTTP 请求和响应了



##### 总结

- RSA 秘钥协商算法不支持 前向保密，ECDHE 算法支持
- RSA 算法必须等待四次握手结束才能进行数据传输；ECDHE 算法在第四次握手之前就可以了，节省了时间
- ECDHE 算法在 TLS 第二次握手中会发送 Server Key Exchange 消息，RSA 算法没有



### HTTPS 如何优化

****

#### 性能损耗分析

两方面

- TLS 协议握手过程
- 对称加密报文传输

第二个过程来说，主流的对称加密算法 AES，chacha20 性能都很好，有些 CPU 也做了硬件级别的优化

所以主要的优化目标是 TLS 协议握手过程

- 对于 ECDHE 算法，TLS 握手过程中，会动态生成双方的公私钥
- 客户端验证数字证书时，也会访问 CA
- 计算对称加密秘钥



#### 硬件优化

**HTTPS 协议是计算密集型，不是 I/O 密集型，需要更加强大的计算能力（CPU）**

可以选择支持 **AES-NI 特性的 CPU，它优化了 AES 算法**



#### 软件优化

主要就是软件升级



#### 协议优化

- 密钥交换算法优化

  RSA 算法必须进行 四次握手（TLS 1.2），而且不具备前向安全性

  ECDHE 算法可以在 三次握手之后就进行 HTTP 对称加密信息传输，由 2 RTT 减少到了 1 RTT，具备前向安全性

  不同的椭圆曲线性能也不一样，目前最快的是 **x25519 曲线**

  握手之后的对称加密算法中，**AES_128_GCM 比 AES_256_GCM** 快一些，安全性差一些

- TLS 升级

  TLS 1.3 只需要进行 3 次握手，1 RTT

  1.2 中，客户端向服务端发送 公钥是在 第三次握手；1.3 优化的就是这个过程，直接把 第三次握手的动作放入到了第一次握手中

  具体操作就是：客户端在 Client Hello 消息中带上了客户端支持的椭圆曲线，以及这些椭圆曲线对应的公钥

  服务端接收到之后，选定一个椭圆曲线，然后把对应的公钥返回给客户端，这样，1 RTT 内，双方就都有了生成对称加密秘钥的全部信息了

  **1.3 不在支持 RSA 和 DH 密钥交换算法，只支持 ECDHE 算法**



#### 证书优化

- 证书传输

  对于服务 器的证书应该选择椭圆曲线（ECDSA）证书，⽽不是 RSA 证书，因为在相同安全强度下， ECC 密钥⻓度⽐ RSA 短的多

- 证书验证

  客户端在验证证书时，是个复杂的过程，会⾛证书链逐级验证，验证的过程不仅需要「⽤ CA 公钥解密证书」以及 「⽤签名算法验证证书的完整性」，⽽且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性

  这个访问过程是 HTTP 访问，因此⼜会产⽣⼀系列⽹络通信的开销，如 DNS 查询、建⽴连接、收发数据等

  > **CRL**：证书吊销列表，这个列表是由 CA 定期更新，列表内容都是被撤销信任的 证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的

  CRL 有两个问题：

  - 实时性差
  - 随着吊销证书的增多，列表会越来越⼤，下载的速度就会越慢

  > **OCSP**：CRL 的替代品，名为在线证书状态协议，来查询证书的有效性，它的⼯作⽅式是向 CA 发送查询请求，让 CA 返回证书的有效状态

  OCSP Stapling

  为了解决 OCSP 还需要向 CA 发送请求这一网络开销

  > **OCSP Stapling**：服务器向 CA 周期性地查询证书状态，获得 ⼀个带有时间戳和签名的响应结果并缓存它。当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握⼿过程中发给客户端。由于有签名的存在， 服务器⽆法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询



#### 会话复用

TLS 握手是为了协商对称加密秘钥，如果把首次 TLS 协商的对称加密秘钥缓存起来，下次需要建立 HTTPS 连接直接复用，就可以减少性能损耗，这就是会话复用，分两种：

- Session ID

  客户端和服务器⾸次 TLS 握⼿连接后，双⽅会在内存缓存会话密钥，并⽤唯⼀的 Session ID 来标识，Session ID 和会话密钥相当于 key-value 的关系

  客户端再次连接时，hello 消息中会带上 Session ID，服务器收到后就会从内存找，找到就直接恢复，只用 1 个 RTT，但是会定期失效

  缺点：

  - 服务器的内存压力会越来越大
  - 多台服务器负载均衡，客户端再次连接不⼀定会命中上次访问过的服务 器，于是还要⾛完整的 TLS 握⼿过程

- Session Ticket

  服务器不再缓存每个客户端的会话密钥，⽽是把缓存的⼯ 作交给了客户端，类似于 HTTP 的 Cookie

  首次建立 TLS 连接后，服务器会加密 会话密钥 作为 Ticket 发给客户端，交给客户端缓存该 Ticket

  客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上⼀次的会话密钥，然后验证有效期， 如果没问题，就可以恢复会话了，开始加密通信

**Session ID 和 Session Ticket 都不具备前向安全性**

- Pre-shared Key

  TLS 1.3 会话复用的方式，重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这样只需要 0 RTT 就能重连



## TCP

------

### TCP 基本认识

****

#### TCP 头格式

重要字段有以下几个：

- 序列号：通过 SYN 包传递给接收端主机，**用来解决网络包乱序问题**
- 确认应答号：下一次 **期望** 收到的数据的序列号，发送端收到这个，就默认这个序列号之前的数据都被正常接收了，**用来解决不丢包问题**
- 控制位：
  - `ACK`：**确认应答字段生效**，TCP 规定除了最初建立连接时的 SYN 包之外，该位必须设置为 1
  - `RST`：TCP 连接出现异常，必须强制断开连接
  - `SYN`：希望建立连接，并在其 序列号 字段进行序列号初始值的设定
  - `FIN`：希望断开连接



#### 为什么需要 TCP 协议？TCP工作在哪一层？

因为 IP 协议 不能保证网络包的交付，网络包的有序交付，也不能保证网络包中数据的完整性

TCP 协议是工作在 **传输层** 的 **可靠的** 数据传输服务，他能确保接收端接收的网络报是 **无损坏、无间隔、非冗余、按序的**



#### 什么是 TCP？

> TCP 是 **面向连接的、可靠地、基于字节流的传输层通信协议**

- 面向连接：必须是 **一对一连接**，不能像 UDP 一样 **一个主机同时向多个主机发送消息**
- 可靠的：无论网络链路出现了怎样的链路变化，TCP 都可以保证一个报文一定能到达接收端
- 字节流：消息是 没有边界的，所以无论多大都可以传输。消息也是有序的，前一个消息没有收到，即便先收到后一个消息，应用层也不能处理，并且重复的报文会自动丢弃



#### 什么是 TCP 连接？

> 用于保证可靠性和流浪控制维护的某些状态信息，这些信息的组合，包括 socket、序列号和窗口大小，成为连接

- Socket：由 IP 地址和端口号组成
- 序列号：用来解决乱序等问题
- 窗口大小：控制流量



#### 如何唯一确定一个 TCP 连接？

TCP 四元组

- 源地址
- 源端口
- 目的地址
- 目的端口

源地址和目的地址的字段（32位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机

源端口和目的端口的字段（16位）在 TCP 头部中，作用是告诉 TCP 协议应该把报文发送给哪个进程



#### 有一个 IP 的服务器监听了一个端口，它的 TCP 最大连接数是多少？

服务端的一个接口可以给多个客户端提供服务

理论上讲：最大 TCP 连接数 = 客户端的 IP 数 * 客户端的端口数

IPV4，客户端的 IP 数最多为 2^32，客户端的端口数最多为 2^16

也就是说，**理论上讲，服务器单机的最大 TCP 连接数约为 2^48**

实际上，服务端最大并发 TCP 连接数远不能达到理论上限

- **文件描述符** 限制：Socket 都是文件，所以首先要通过 `ulimit` 配置未见描述符的数目
- **内存限制**：每个 TCP 连接都要占用一定的内存，操作系统的内存是有限的



#### 什么是 UDP？

UDP 不提供复杂的控制机制，利用 IP 提供面向 **无连接** 的通信服务

UPD 的头部格式如下：

- 源端口号：(2 字节)
- 目标端口号：(2 字节) 源端口和目标端口主要是 告诉 UDP 协议该把报文发送给哪个进程
- 包长度：(2 字节) 该字段保存了 UDP 首部和数据的长度之和
- 校验和：(2 字节) 为了提供可靠地 UDP 首部和数据



#### UDP 和 TCP 有什么区别？

- 连接

  - TCP 是 **面向连接** 的传输层协议，传输数据前先要建立连接
  - UDP 是 **无连接** 的，即刻传输数据

- 服务对象

  - TCP 是 **一对一** 的两点服务，一条连接只有两个端点
  - UDP 支持 **一对一，一对多，多对多** 的交互

- 可靠性

  - TCP 是 **可靠的**，数据可以 **无差别、不丢失、不重复、按序到达**
  - UDP 是 **不可靠的**，不保证可靠交付

- 拥塞控制、流量控制

  - TCP 有 **拥塞控制和流量控制**，保证数据传输的安全性
  - UDP 没有，**网络非常拥堵也不影响 UDP 的发送速率**

- 首部开销

  - TCP 的首部在不使用 选项 字段时，是 20 字节，使用了更长
  - UDP 首部就 8 字节，固定不变

- 传输方式

  - TCP 是**流式传输，没有边界，但保证顺序和可靠**
  - UDP 是一个包一个包发送的，**有边界，有可能丢包和乱序**

- 分片不同

  > MSS：最大报文长度，TCP 的一个选项头部，用于在TCP连接建立时，收发双方协商通信时每一个报文段所能承载的最大数据长度（不包括文段头）

  > MTU：包或帧的最大长度，一般以字节记

  

  - TCP 的数据大小如果大于 MSS，就会 **在传输层分片**；目标主机收到后，也在传输层组装 TCP 数据包，**如果中途丢失了⼀个分⽚，只需要传输丢失的这个分⽚**
  - UDP 的数据大小如果大于 MTU，就会在 **IP 层分片**，⽬标主机收到后，在 IP 层组装完数据，接着再传给传输层；**如果中途丢了⼀个分⽚，在实现可靠传输的 UDP 时就需要重传所有的数据包，这样传输效率⾮常差，所以通常 UDP 的报⽂应该小于 MTU**



#### UDP 和 TCP 的应用场景分别是什么？

- TCP 是面向连接的，能保证数据的 **可靠性** 交付
  - `FTP` 文件传输
  - `HTTP` /  `HTTPS`
- UDP 面向无连接，可以**随时发送数据**，UDP 本身的处理**简单高效**
  - 包总量较少的通信，如 `DNS`、`SNMP` 等
  - 视频、音频等多媒体通信
  - 广播通信



#### 为什么 UDP 头部没有 首部长度 字段，而 TCP 头部有

原因是 TCP 的头部长度是可变的，至少 20 字节；UDP 的头部长度是固定的 8 字节



#### 为什么 UDP 头部有 包长度 字段，而 TCP 头部没有

TCP 数据长度 = IP 总长度 - IP 首部长度 - TCP 首部长度

IP 总长度 和 IP 首部长度是在 IP 首部中知道的；TCP 的 首部长度 是在 TCP 首部中知道的。所以就可以求得 TCP 数据的长度了

按理来说，UDP 也是不需要 包长度 字段的

但是，**为了⽹络设备硬件设计和处理⽅便，⾸部⻓度需要是 44 字节的整数倍**

如果去掉 UDP 包长度 字段，UDP 的首部长度就不是 4 字节的整数倍了



### TCP 连接建立

****

#### TCP 三次握手过程和状态变迁

- 一开始，客户端和服务端都处于 `CLOSED` 状态，先由服务端监听某个端口，服务端变更为 `LISTEN` 状态
- **三次握手的第一个报文**：**客户端会随机初始化序列号（`client_isn`），置于 TCP 首部的 序列号字段，同时 把 `SYN` 标志位置为 1**，表示这是一个 `SYN` 报文，然后发送给服务端，向服务端请求建立连接，**SYN 报文不包含应用层数据**，然后 **客户端变更为 `SYN-SENT` 状态**
- **三次握手的第二个报文**：服务端接收到 `SYN` 报文， **服务端也会初始化序列号（server_isn），置于 TCP 首部的 序列号字段，把 `client-isn + 1` 填入 确认应答号 字段，同时把 SYN 和 ACK 标志位置为 1**，表示 这是一个 `SYN + ACK` 报文，然后发送给客户端，**该报文也不包含 应用层数据，之后服务端变更为 `SYN-RCVD` 状态**
- **三次握手的第三个报文**：客户端收到 `SYN-ACK` 报文后，还要向服务端发送最后一个报文。**把 `server-isn + 1` 填入 确认应答号 字段，把 `ACK` 标志位置为 1**，最后发送给服务端，**这次报文可以携带 客户端的数据，之后客户端状态变更为 `ESTABLISHED`**
- 服务端收到客户端的应答报文后，**也把状态变更为 ESTABLISHED**

**前两次握手不能携带应用层数据，第三次可以**



#### 为什么是三次握手？不是两次、四次？

TCP 是 **面向连接的**，需要维护 Socket、序列号、窗口大小这些信息，以建立连接

以下是三次握手的原因：

- 三次握手才可以阻止重复历史连接的初始化（主要原因）

  - **三次握手的首要原因就是防止旧的重复连接初始化造成混乱**

  场景：

  客户端连续发送多次 SYN 建⽴连接的报⽂，在⽹络拥堵情况下：

  - 一个 **旧 SYN 报文** 比 **新 SYN 报文** 先到达服务端
  - 然后服务端就会响应一个 **SYN + ACK 报文（对应 旧 SYN 报文）**
  - 客户端收到后，根据自身上下文判断这是一个历史连接（我猜可能是用 确认应答号 字段 判断的），**客户端就会发送 `RST` 报文给服务端，表示终止这次连接**

  **如果两次握手，就没有 判断是否是历史连接这一操作的时间，三次握手则可以在客户端准备发送第三次报文的时候，判断当前连接是否是历史连接**

  - 是历史连接，第三次握手就发送 `RST` 报文，表示终止连接
  - 不是历史连接，发送 `ACK` 报文，表示建立 TCP 连接成功

- 三次握手才可以同步双方的初始序列号

  序列号的作用：

  - 接收方可以去除重复的数据
  - 接收方可以根据数据包的序列号按序接收
  - 可以标识发送出去的数据包中，哪些是成功被对方收到的

  TCP 协议的通信双方，都必须维护一个自己的 序列号，以保证数据的可靠传输

  **一来一回，才能确保双方的初始序列号能同步**

  所以，理论上来讲：

  - 第一个来回，客户端发送自己的初始序列号，给服务端同步，服务端发送 确认应答号，表示服务端已同步客户端的初始序列号
  - 第二个来回，服务端发送自己的初始序列号，给客户端同步，客户端发送确认应答号，表示客户端已同步服务端的初始序列号

  这样的话，应该是四次握手，**但是服务端发送确认应答号和初始序列号两步可以优化成一步，所以就成了三次握手**

  **两次握手只能确保同步一方的初始序列号**

- 三次握手才可以避免浪费资源

  如果 只有  两次握手，服务端每接收到一个 `SYN` 报文，就必须建立一个连接，**因为服务端不知道客户端是否收到了自己的 `ACK` 确认建立连接报文**

  这样的话，一旦客户端发送的 `SYN` 报文在网络中阻塞了，客户单就会重复发送多次 `SYN` 报文，服务端就会建立多个 TCP 连接，但这些都是冗余的，会造成资源浪费

总结：

**TCP 为什么是三次握手：是因为三次握手能防止历史连接的初始化；同步连接双方的初始序列号；减少不必要的资源浪费**

**两次握手不能做到以上三点；四次握手不需要，因为三次足够了**



#### 为什么初始序列号要求每次连接随机生成？

为了确认是历史连接的报文，还是当前连接的报文，防止接收数据紊乱；防止黑客伪造相同序列号的 TCP 报文被对方接收



#### 初始序列号 是如何随机产生的？

ISN = M + F

- M 是一个计时器，每隔 4 ms 加一
- F 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口随机生成



#### IP 层会分片，为什么 TCP 还需要 MSS？

网络包由以下几部分组成：

1. 报头/起始帧分解符
2. MAC头部
3. IP头部
4. TCP头部
5. 数据
6. FCS

其中 MTU 就是 3/4/5；MSS 就是 5

- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节
- `MSS`：除去 IP 和 TCP 头部，一个网络包所能容纳的 TCP 数据的最大长度

IP 层没有超时重传机制，传输层的 TCP 负责超时和重传

**只用 IP 分片的话，一个分片丢失，整个 TCP 报文都要重传。**当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，就不会响应 ACK 给对方，发送方的 TCP 超时后，就会 **重发整个 TCP 报文（头部 + 数据）。**这样是很低效的

所以，TCP 协议在 **建立连接的时候（TCP 第一次握手，SYN 报文）通常会协商MSS 值**，当超过这个大小的时候，**TCP 报文就会在 传输层分片，这个 IP 包的长度必然小于 MTU，所以就不会在 IP 层分片**，这样，**如果某个 TCP 分片丢失，那只需要重传这一个 MSS 的数据，增加了重传的效率**



#### 什么是 SYN 攻击？

TCP 建立连接要三次握手，假如攻击者短时间内伪造不同 IP 地址发送 `SYN` 报文，服务端每接收一个 `SYN` 报文，就会进入 `SYN_RCVD` 状态，但是服务端发送出去的 `SYN + ACK` 报文，无法得到客户端的 `ACK` 应答，过多这样的建立连接请求就会占满服务端的 `SYN` 接收队列（未连接队列），服务器就不能正常工作了



### TCP 连接断开

****

#### TCP四次挥手过程和状态变迁

TCP 连接 **双方都可以主动断开连接**，断开连接后，主机中的资源会被释放

> MSL（Maximum Segment Lifetime）最大报文生存时间：它是任何报文段被丢弃前在网络内的最长时间。RFC 793指出MSL为2分钟，现实中常用30秒或1分钟

- 客户端打算关闭连接，会发送一个 `FIN` 报文，`FIN` 标志位会置为 1，客户端变更为 `FIN_WAIT_1` 状态
- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，服务端变更为 `CLOSED_WAIT` 状态
- 客户端接收到服务端的 `ACK` 应答报文后，变更为 `FIN_WAIT_2` 状态
- 服务端处理完数据后，也会向客户端发送 `FIN` 报文，服务端变更为 `LAST_ACK` 状态
- 客户端收到服务端的 `FIN` 报文后，回应一个 `ACK` 报文，客户端变更为 `TIME_WAIT` 状态
- 服务端收到 `ACK` 报文后，状态变更为 `CLOSED`，服务端完成连接的关闭
- 客户端在经过 `2MSL` 后，自动变更为 `CLOSED` 状态

**每个方向都需要一个 FIN 和 一个 ACK，因此通常称为 四次挥手**

**主动关闭连接的，才有 `TIME_WAIT` 状态**



#### 为什么挥手要四次？

假设 A 方要关闭，B 方被关闭

- A方先发送 `FIN` 报文，表示 **我不再发送数据了，但是可以接收**
- B方接收到 `FIN` 报文后，回应一个 `ACK` 报文，**但是 B 方可能还有数据要处理，发送给 A 方**，等 B 方不再需要发送数据给 A 方了，就发送一个 `FIN` 报文，**表示同意关闭连接**

这样来看，被动的一方通常可能会有数据需要再发送给主动的一方，所以 `ACK` 和 `FIN` 报文通常会分开发送，所以比三次握手多了一次



#### 为什么 `TIME_WAIT` 等待的时间是 2 MSL

> TTL：是 IP 数据报可以经过的最⼤路由数，每经过⼀个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报⽂通知源主机

⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包 被接收⽅处理后⼜会向对⽅发送响应，所以⼀来⼀回需要等待 2 倍的时间

2MSL 的时间是**从客户端接收到 FIN 后发送 ACK 开始计时的**。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端⼜接收到了服务端᯿发的 FIN 报⽂，那么 2MSL 时间将重新计时



### TCP 重传

****

TCP 连接中，发送端发出一条消息后，接收端正常接收到的话会回复一个确认应答消息，表示已收到，这个机制是依靠序列号实现的

如果数据在传输过程中丢失了的话， TCP 就会用重传机制解决

常见的重传机制有：

- 超时重传
- 快速重传
- SACK
- D-SACK

#### 超时重传

发送数据时，设定一个定时器，超过指定时间后，如果没有收到接收端的 `ACK（确认应答）` 报文，就会触发 **超时重传**

两种情况会触发超时重传：

- 数据包丢失
- 确认应答丢失

##### 超时时间设置为多少合适？

超时重传时间是以 `RTO` （Retransmission Timeout 超时重传时间）表示的

- `RTO` 较大：如果 RTO 太大了，重发间隔就会很大，那样性能就会很差
- `RTO` 较小：如果 RTO 太小的话，可能导致没有丢包就触发超时重传了，性能也有影响

所以，`RTO` 不能太大，也不能太小，**边界就是 `RTT`，`RTO` 应当略微大于 RTT，这样既不会在没有丢包的时候触发超时重传，也不会因为重传间隔太大而导致性能很差**

**因为 RTT 是波动变化的，所以 RTO 也应该是动态变化的**

每当遇到⼀次超时重传的时候，都会将下⼀次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送



#### 快速重传

快速重传不是根据时间判断是否重传的，**而是根据数据**

简单来说，如果发送方某个数据包丢失，即使它继续发送之后的数据包，接收方的确认应答报文中的 **确认应答号** 依旧是未接收到的第一个数据包，**连续三次收到相同的 确认应答报文后，就会触发快速重传**

快速重传就是为了解决超时时间的问题，但是快速重传也有问题，**重传的时候，是重传之前的一个，还是重传所有？因为发送端并不知道三个相同的确认应答报文是哪个数据包传回来的**



#### SACK 方法

为了解决不知道该重传哪些 TCP 报文，就实现了 `SACK` 方法（ `Selective Acknowledgment` 选择性确认）

实现手段就是：**在 TCP 头部的 选项 字段中加一个 `SACK`，用来存储缓存地图，发送方就可以知道哪些数据没有收到了**

**发送方连续三次收到同样的 ACK 确认报文，就会触发快速重传机制，`SACK` 信息帮助发送端确认丢失的数据，只重发丢失的数据**

**如果要支持 `SACK`，必须双方都支持**



#### Dumplicate SACK

又称 `D-SACK`，主要使用 `SACK` 哪些数据被重复接收了

场景：

- `ACK` 丢包

  如果触发重传是因为 `ACK `应答报文丢失而触发 **超时重传机制**，那接收方就会收到重复的数据，再次发送的 `ACK` 应答报文中的 `SACK` 就会记录重复收到的记录

- 网络延时

  当因为网络延时而迟迟未到达接收方的数据触发 **快速重传机制**，而重新发送过去，然后这个慢的报文才到达接收方的时候，就会知道是因为 网络延时才触发的快速重传

好处：

- 可以知道是发出去的包丢失了，还是 `ACK` 包丢失了
- 可以知道是不是网络延迟导致数据包迟迟未到达接收方
- 可以知道网络中是不是把数据包复制了



### 滑动窗口

如果每发送一条数据都要进行一次确认应答，必须这样顺序执行的话，通信效率是比较低的（**RTT 越长，通信效率越低**）

为了解决这个，就有了窗口（RTT 较长也不会影响通信效率）

> 窗口大小：无需等待应答，而可以继续发送数据的最大值

> 窗口：窗口实际上是接收方开辟的一块缓存空间，只要窗口还没满，发送方就可以无需等待应答，继续发送数据

假设窗口大小设置为 3 个 TCP 段，那么发送端就可以连续发送 3 个 TCP 报文，**如果中途有 `ACK` 丢失，可以通过下一个确认应答进行确认**



##### 窗口大小由哪方决定？

TCP 头里有一个 `Window（窗口大小）` 字段

这个字段是接收端告诉发送端⾃⼰还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，⽽不会导致接收端处理不过来



- 发送方窗口

  发送方窗口分为四部分：

  - 已发送并收到 `ACK` 确认的数据
  - 已发送但未收到 `ACK` 确认的数据
  - 未发送但总大小在接收方窗口内的数据
  - 未发送但总大小超过了接收方窗口大小的数据

  TCP 滑动窗口方案用三个指针来跟踪四个类别的连接处：

  - `SND.WND`：发送窗口的大小
  - `SND.UNA`：绝对指针，指向的是已发送但未收到确认的第⼀个字节的序列号
  - `SND.NXT`：绝对指针，指向未发送但可发送范围的第⼀个字节的序列号
  - 指向 未发送但总大小超过了接收方窗口大小范围的第一个字节的 是相对指针，由 `SND.UNA` + `SND.WND` 计算得出

- 接收方窗口

  分为三个部分：

  - 已成功接收并确认的数据
  - 未收到数据但在接收方窗口大小范围内的数据
  - 未收到数据而且超出接收方窗口大小范围的数据

  三个部分，两个指针划分：

  - `RCV.WND`：接收窗口大小
  - `RCV.NXT`：指向第二部分的第一个字节
  - 指向第三部分的第一个字节的是相对指针，由 `RCV.NXT` + `RCV.WND` 计算得出

- 二者大小并不完全相等



## IP

------

### 基本认识

****

IP 在 TCP/IP 参考模型中处于网络层

网络层的主要作用是：实现主机与主机之间的通信（点对点通信）

#### 网络层与数据链路层有什么关系？

网络层（IP）的作用是主机之间通信用，而数据链路层（MAC）的作用是实现直连的两个设备之间通信，IP 负责在没有直连的两个网络间通信

**源IP地址和⽬标IP地址在传输过程中是不会变化的，只有源 MAC 地址和⽬标 MAC ⼀直在变化**



## 网络综合

------

#### 前端鉴权

------

基于 HTTP **无状态** 的特性，在某些场景下，我们需要维护状态，最典型的就是用户登录，用以访问某些信息

无状态，但是需要维护状态，又不能频繁发送 HTTP 请求，那就只能把状态保存在前端：

- 挂载到全局变量上，刷新页面就没了
- cookie、localStorage 等里，方便访问，而且不会随着刷新过期

通常是使用 cookie，因为我们保存状态的目的也是再接下来的请求中，让服务端能够识别我们，那就需要在 HTTP 请求中带上 状态信息，cookie 不需要我们手动操作，设置好之后，会在对应的 Domain/Path 的请求发送时自动带上

##### 应用方案：服务端 session

- 客户端发送账号密码，服务端验证
- 验证成功就把用户状态存为 session，生成一个 sessionId
- 把 sessionId 放到 Set-Cookie 响应头中，返回给客户端
- 之后客户端再请求业务接口，sessionId 就会自动添加到 Cookie 中
- 服务端查询 sessionId，验证 session，通过就正常处理业务，返回结果

session的过期与销毁：可以在 session 中加入过期时间，到期就删除库中的数据



##### 应用方案：Token

session 的方案有一个痛点，就是需要服务端维护 session 信息，但这其实是没有必要的，session 中的信息其实可以直接反应在 cookie 中，然后在业务代码中直接验证

- 客户端登录，服务端验证用户信息
- 把必要的信息，比如用户身份信息，状态信息，token 过期时间等，编码成 token，然后放到 Set-Cookie 响应头中，返回给客户端
- 之后客户端再请求业务接口，token 就会自动添加到 Cookie 中
- 验证 token，通过就正常处理业务

**防篡改**

如果 token 只是由信息直接简单编码，那就很容易被人得知信息，进而伪造 token进行非法访问

如果 token 涉及到敏感权限，就要想办法避免这样的事

解决方案：给 token 加签名，然后通过加密算法复杂编码



##### JWT

token 的防篡改会额外增加 cookie 的体积，加大服务端的压力，JWT 就是解决这个问题的

> JSON Web Token (JWT) 是一个开放标准，定义了一种传递 JSON 信息的方式。这些信息通过数字签名确保可信



##### refresh token

业务接口用来鉴权的 token 被称为 access token，权限敏感的话，我们就希望有效期尽可能短，防止被盗用，但是太短的话，又会造成频繁验证身份

解决办法就是，用另一个 token 生成 access token，我们称之为 refresh token

- 客户端登录，服务端验证用户信息
- 验证通过，生成 refresh token，生成 access token，返回给客户端
- 之后客户端再请求业务接口，access token 就会自动添加到 Cookie 中
- 如果 access token 过期，就请求生成新的 access token
- 获得新的 access token 之后再请求业务接口

**refresh token 的作用就是，access token 频繁的过期生成，不再需要与数据库进行交互，这就节省了大量时间**



##### session 与 token

在浏览器端，可以用 cookie，如果没有 cookie 的话，就用别的前端存储方案，而且，**cookie 容易引发CSRF 攻击**

session 和 token 在浏览器端的状态存储，一般都选择 cookie

服务端来说：

- session 存数据，请求只需要携带 sessionId，可以减少请求头部信息，但是需要与 库交互，验证 session
- token 不存数据，所有的数据就在 请求 中，不需要服务端整套的解决方案和分布式处理，也减少了与库交互带来的性能开销

##### 单点登录

同域下的客户端/服务端认证系统中，客户端携带凭证，以维护状态信息

但是如果业务系统过大，分散到了不同的域名下，就需要 **一次登录，到处使用** 的能力，这就是 **单点登录**

**注意，只有 SSO 系统有登陆功能，其他系统负责各自的业务功能**

- 如果整套业务系统都在一个顶级域名下，那只需要把 cookie 的 domain 字段设置为公用的顶级域名，就实现了 单点登录

- 如果并不都在一个主域名下，就需要实现 单点登录系统（SSO）了

  非浏览器（不需要考虑跨域限制）

  - 客户端访问 A 系统，未登录状态，重定向到 SSO 系统
  - SSO 系统也是未登录状态，用户填写信息进行登录，如果验证成功，就返回给客户端一个 登录凭证（ticket）
  - 因为没有跨域限制，所以请求其他系统的时候，该登录凭证是通用的，**但是，验证登陆凭证是在 SSO 系统**

  浏览器（有跨域限制）

  - 客户端访问 A 系统，未登录状态，重定向到 SSO 系统
  - SSO 系统也是未登录状态，用户填写信息进行登录，如果验证成功，就可以完成 **客户端与SSO 系统的鉴权** 了，即客户端存储了 SSO 系统这个 域下的 cookie
  - SSO 系统完成鉴权后，还会生成一个 ST（Service Ticket），然后跳转回 A 系统，并将 ST 作为参数传递给 A 系统
  - A 系统拿到 ST 后，向 SSO 系统发送请求，验证 ST
  - 验证通过， **A 系统将登录状态写入到客户端 A 系统域下的 cookie**，这样，A 系统和客户端的鉴权也完成了
  - 不同系统的 cookie 肯定不同，因为有跨域限制

  不管是不是在浏览器，每次向业务系统发送请求时，**后台向 SSO 发送请求验证身份都是必要的**，如果不进行验证的话，是可以进行伪造信息的

